{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rmCPmqFL6hCQ"
   },
   "source": [
    "# 📊 Dataset Maker by Hollowstrawberry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2023-08-10T18:10:15.022036Z",
     "iopub.status.busy": "2023-08-10T18:10:15.021507Z",
     "iopub.status.idle": "2023-08-10T18:10:16.491077Z",
     "shell.execute_reply": "2023-08-10T18:10:16.489453Z",
     "shell.execute_reply.started": "2023-08-10T18:10:15.021991Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME=\"Ubuntu\"\n",
      "VERSION=\"20.04.5 LTS (Focal Fossa)\"\n",
      "ID=ubuntu\n",
      "ID_LIKE=debian\n",
      "PRETTY_NAME=\"Ubuntu 20.04.5 LTS\"\n",
      "VERSION_ID=\"20.04\"\n",
      "HOME_URL=\"https://www.ubuntu.com/\"\n",
      "SUPPORT_URL=\"https://help.ubuntu.com/\"\n",
      "BUG_REPORT_URL=\"https://bugs.launchpad.net/ubuntu/\"\n",
      "PRIVACY_POLICY_URL=\"https://www.ubuntu.com/legal/terms-and-policies/privacy-policy\"\n",
      "VERSION_CODENAME=focal\n",
      "UBUNTU_CODENAME=focal\n",
      "FIFTYONE_DATABASE_DIR \n"
     ]
    }
   ],
   "source": [
    "!cat /etc/os-release\n",
    "!export FIFTYONE_DATABASE_DIR=/storage/fiftyone/db\n",
    "!echo \"FIFTYONE_DATABASE_DIR ${FIFTYONE_DATABASE_DIR}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *️⃣ Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-10T18:10:20.798013Z",
     "iopub.status.busy": "2023-08-10T18:10:20.797614Z",
     "iopub.status.idle": "2023-08-10T18:16:35.723398Z",
     "shell.execute_reply": "2023-08-10T18:16:35.722217Z",
     "shell.execute_reply.started": "2023-08-10T18:10:20.797964Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/notebooks\n",
      "Collecting accelerate==0.15.0\n",
      "  Downloading accelerate-0.15.0-py3-none-any.whl (191 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m191.5/191.5 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.9/dist-packages (from accelerate==0.15.0) (1.12.1+cu116)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from accelerate==0.15.0) (23.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.9/dist-packages (from accelerate==0.15.0) (5.9.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from accelerate==0.15.0) (1.23.4)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.9/dist-packages (from accelerate==0.15.0) (5.4.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.4.0->accelerate==0.15.0) (4.4.0)\n",
      "Installing collected packages: accelerate\n",
      "Successfully installed accelerate-0.15.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting transformers==4.26.0\n",
      "  Downloading transformers-4.26.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m51.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers==4.26.0) (4.64.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers==4.26.0) (5.4.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers==4.26.0) (0.12.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers==4.26.0) (2022.10.31)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers==4.26.0) (0.12.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers==4.26.0) (23.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers==4.26.0) (1.23.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers==4.26.0) (3.9.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers==4.26.0) (2.28.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.26.0) (4.4.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.26.0) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers==4.26.0) (2019.11.28)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.26.0) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers==4.26.0) (2.8)\n",
      "Installing collected packages: transformers\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.21.3\n",
      "    Uninstalling transformers-4.21.3:\n",
      "      Successfully uninstalled transformers-4.21.3\n",
      "Successfully installed transformers-4.26.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting ftfy==6.1.1\n",
      "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.9/dist-packages (from ftfy==6.1.1) (0.2.6)\n",
      "Installing collected packages: ftfy\n",
      "Successfully installed ftfy-6.1.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting albumentations==1.3.0\n",
      "  Downloading albumentations-1.3.0-py3-none-any.whl (123 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.5/123.5 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.9/dist-packages (from albumentations==1.3.0) (5.4.1)\n",
      "Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.9/dist-packages (from albumentations==1.3.0) (1.23.4)\n",
      "Requirement already satisfied: scikit-image>=0.16.1 in /usr/local/lib/python3.9/dist-packages (from albumentations==1.3.0) (0.19.3)\n",
      "Collecting opencv-python-headless>=4.1.1\n",
      "  Downloading opencv_python_headless-4.8.0.76-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 MB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting qudida>=0.0.4\n",
      "  Downloading qudida-0.0.4-py3-none-any.whl (3.5 kB)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from albumentations==1.3.0) (1.9.2)\n",
      "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.9/dist-packages (from qudida>=0.0.4->albumentations==1.3.0) (1.1.2)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from qudida>=0.0.4->albumentations==1.3.0) (4.4.0)\n",
      "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.9/dist-packages (from scikit-image>=0.16.1->albumentations==1.3.0) (3.0)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.9/dist-packages (from scikit-image>=0.16.1->albumentations==1.3.0) (2023.1.23.1)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-image>=0.16.1->albumentations==1.3.0) (1.4.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from scikit-image>=0.16.1->albumentations==1.3.0) (23.0)\n",
      "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,!=8.3.0,>=6.1.0 in /usr/local/lib/python3.9/dist-packages (from scikit-image>=0.16.1->albumentations==1.3.0) (9.2.0)\n",
      "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.9/dist-packages (from scikit-image>=0.16.1->albumentations==1.3.0) (2.25.0)\n",
      "Requirement already satisfied: joblib>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations==1.3.0) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations==1.3.0) (3.1.0)\n",
      "Installing collected packages: opencv-python-headless, qudida, albumentations\n",
      "Successfully installed albumentations-1.3.0 opencv-python-headless-4.8.0.76 qudida-0.0.4\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting opencv-python==4.7.0.68\n",
      "  Downloading opencv_python-4.7.0.68-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (61.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.8/61.8 MB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.9/dist-packages (from opencv-python==4.7.0.68) (1.23.4)\n",
      "Installing collected packages: opencv-python\n",
      "  Attempting uninstall: opencv-python\n",
      "    Found existing installation: opencv-python 4.6.0.66\n",
      "    Uninstalling opencv-python-4.6.0.66:\n",
      "      Successfully uninstalled opencv-python-4.6.0.66\n",
      "Successfully installed opencv-python-4.7.0.68\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting einops==0.6.0\n",
      "  Downloading einops-0.6.0-py3-none-any.whl (41 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.6/41.6 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: einops\n",
      "Successfully installed einops-0.6.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting diffusers==0.10.2\n",
      "  Downloading diffusers-0.10.2-py3-none-any.whl (503 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m503.1/503.1 kB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.9/dist-packages (from diffusers==0.10.2) (6.0.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.10.0 in /usr/local/lib/python3.9/dist-packages (from diffusers==0.10.2) (0.12.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from diffusers==0.10.2) (3.9.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from diffusers==0.10.2) (1.23.4)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.9/dist-packages (from diffusers==0.10.2) (9.2.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from diffusers==0.10.2) (2.28.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from diffusers==0.10.2) (2022.10.31)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.10.0->diffusers==0.10.2) (4.64.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.10.0->diffusers==0.10.2) (4.4.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.10.0->diffusers==0.10.2) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.10.0->diffusers==0.10.2) (5.4.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata->diffusers==0.10.2) (3.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/dist-packages (from requests->diffusers==0.10.2) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->diffusers==0.10.2) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->diffusers==0.10.2) (2019.11.28)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->diffusers==0.10.2) (2.8)\n",
      "Installing collected packages: diffusers\n",
      "Successfully installed diffusers-0.10.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting pytorch-lightning==1.9.0\n",
      "  Downloading pytorch_lightning-1.9.0-py3-none-any.whl (825 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m825.8/825.8 kB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting lightning-utilities>=0.4.2\n",
      "  Downloading lightning_utilities-0.9.0-py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: packaging>=17.1 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning==1.9.0) (23.0)\n",
      "Collecting torchmetrics>=0.7.0\n",
      "  Downloading torchmetrics-1.0.3-py3-none-any.whl (731 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.6/731.6 kB\u001b[0m \u001b[31m52.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: fsspec[http]>2021.06.0 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning==1.9.0) (2023.1.0)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning==1.9.0) (4.4.0)\n",
      "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning==1.9.0) (1.12.1+cu116)\n",
      "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning==1.9.0) (1.23.4)\n",
      "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning==1.9.0) (4.64.1)\n",
      "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning==1.9.0) (5.4.1)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.9/dist-packages (from fsspec[http]>2021.06.0->pytorch-lightning==1.9.0) (3.8.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from fsspec[http]>2021.06.0->pytorch-lightning==1.9.0) (2.28.2)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning==1.9.0) (2.1.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning==1.9.0) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.9/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning==1.9.0) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning==1.9.0) (1.8.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning==1.9.0) (18.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning==1.9.0) (6.0.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning==1.9.0) (1.3.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning==1.9.0) (2.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning==1.9.0) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning==1.9.0) (2019.11.28)\n",
      "Installing collected packages: lightning-utilities, torchmetrics, pytorch-lightning\n",
      "Successfully installed lightning-utilities-0.9.0 pytorch-lightning-1.9.0 torchmetrics-1.0.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting bitsandbytes==0.35.0\n",
      "  Downloading bitsandbytes-0.35.0-py3-none-any.whl (62.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 MB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: bitsandbytes\n",
      "Successfully installed bitsandbytes-0.35.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting tensorflow==2.11.0\n",
      "  Downloading tensorflow-2.11.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (588.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m588.3/588.3 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting tensorboard<2.12,>=2.11\n",
      "  Downloading tensorboard-2.11.2-py3-none-any.whl (6.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m57.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hCollecting flatbuffers>=2.0\n",
      "  Downloading flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.11.0) (3.3.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.11.0) (0.30.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.11.0) (23.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.11.0) (15.0.6.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.11.0) (1.6.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.11.0) (66.1.1)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.11.0) (3.8.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from tensorflow==2.11.0) (1.14.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.11.0) (0.2.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.11.0) (1.51.1)\n",
      "Collecting tensorflow-estimator<2.12,>=2.11.0\n",
      "  Downloading tensorflow_estimator-2.11.0-py2.py3-none-any.whl (439 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m439.2/439.2 kB\u001b[0m \u001b[31m49.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.11.0) (0.4.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.11.0) (2.2.0)\n",
      "Collecting keras<2.12,>=2.11.0\n",
      "  Downloading keras-2.11.0-py2.py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m55.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.11.0) (1.23.4)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.11.0) (1.14.1)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.11.0) (3.19.6)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.11.0) (1.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.11.0) (4.4.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.9/dist-packages (from astunparse>=1.6.0->tensorflow==2.11.0) (0.35.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (2.2.2)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (1.8.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (0.4.6)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (2.28.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (0.6.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (3.4.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (2.16.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (0.2.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (5.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (4.7.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (6.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (2019.11.28)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (2.8)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (2.1.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (3.11.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (3.2.2)\n",
      "Installing collected packages: flatbuffers, tensorflow-estimator, keras, tensorboard, tensorflow\n",
      "  Attempting uninstall: flatbuffers\n",
      "    Found existing installation: flatbuffers 1.12\n",
      "    Uninstalling flatbuffers-1.12:\n",
      "      Successfully uninstalled flatbuffers-1.12\n",
      "  Attempting uninstall: tensorflow-estimator\n",
      "    Found existing installation: tensorflow-estimator 2.9.0\n",
      "    Uninstalling tensorflow-estimator-2.9.0:\n",
      "      Successfully uninstalled tensorflow-estimator-2.9.0\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: keras 2.9.0\n",
      "    Uninstalling keras-2.9.0:\n",
      "      Successfully uninstalled keras-2.9.0\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.9.1\n",
      "    Uninstalling tensorboard-2.9.1:\n",
      "      Successfully uninstalled tensorboard-2.9.1\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.9.2\n",
      "    Uninstalling tensorflow-2.9.2:\n",
      "      Successfully uninstalled tensorflow-2.9.2\n",
      "Successfully installed flatbuffers-23.5.26 keras-2.11.0 tensorboard-2.11.2 tensorflow-2.11.0 tensorflow-estimator-2.11.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting safetensors==0.2.6\n",
      "  Downloading safetensors-0.2.6-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: safetensors\n",
      "Successfully installed safetensors-0.2.6\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting toml==0.10.2\n",
      "  Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Installing collected packages: toml\n",
      "Successfully installed toml-0.10.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting voluptuous==0.13.1\n",
      "  Downloading voluptuous-0.13.1-py3-none-any.whl (29 kB)\n",
      "Installing collected packages: voluptuous\n",
      "Successfully installed voluptuous-0.13.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting xformers==0.0.20\n",
      "  Downloading xformers-0.0.20-cp39-cp39-manylinux2014_x86_64.whl (109.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.1/109.1 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting pyre-extensions==0.0.29\n",
      "  Downloading pyre_extensions-0.0.29-py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from xformers==0.0.20) (1.23.4)\n",
      "Collecting torch==2.0.1\n",
      "  Downloading torch-2.0.1-cp39-cp39-manylinux1_x86_64.whl (619.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from pyre-extensions==0.0.29->xformers==0.0.20) (4.4.0)\n",
      "Collecting typing-inspect\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Collecting nvidia-cufft-cu11==10.9.0.58\n",
      "  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-cupti-cu11==11.7.101\n",
      "  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m57.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96\n",
      "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch==2.0.1->xformers==0.0.20) (3.9.0)\n",
      "Collecting nvidia-cuda-runtime-cu11==11.7.99\n",
      "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m59.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting sympy\n",
      "  Downloading sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m56.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch==2.0.1->xformers==0.0.20) (3.1.2)\n",
      "Collecting nvidia-nccl-cu11==2.14.3\n",
      "  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting triton==2.0.0\n",
      "  Downloading triton-2.0.0-1-cp39-cp39-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvtx-cu11==11.7.91\n",
      "  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-curand-cu11==10.2.10.91\n",
      "  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusolver-cu11==11.4.0.1\n",
      "  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch==2.0.1->xformers==0.0.20) (3.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu11==11.7.99\n",
      "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusparse-cu11==11.7.4.91\n",
      "  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66\n",
      "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: wheel in /usr/local/lib/python3.9/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1->xformers==0.0.20) (0.35.1)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1->xformers==0.0.20) (66.1.1)\n",
      "Collecting cmake\n",
      "  Downloading cmake-3.27.1-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (26.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.0/26.0 MB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting lit\n",
      "  Downloading lit-16.0.6.tar.gz (153 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.7/153.7 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch==2.0.1->xformers==0.0.20) (2.1.2)\n",
      "Collecting mpmath>=0.19\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m57.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting mypy-extensions>=0.3.0\n",
      "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Building wheels for collected packages: lit\n",
      "  Building wheel for lit (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for lit: filename=lit-16.0.6-py3-none-any.whl size=93583 sha256=78cd9de3a717ab841b1f92db0122724443eefd796569efa23e6d2a810cdc851a\n",
      "  Stored in directory: /root/.cache/pip/wheels/dd/a1/9c/f4e974f934c7a715a884a029e8b2b0b438486e654058fe8c80\n",
      "Successfully built lit\n",
      "Installing collected packages: mpmath, lit, cmake, sympy, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, mypy-extensions, typing-inspect, nvidia-cusolver-cu11, nvidia-cudnn-cu11, pyre-extensions, triton, torch, xformers\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.12.1+cu116\n",
      "    Uninstalling torch-1.12.1+cu116:\n",
      "      Successfully uninstalled torch-1.12.1+cu116\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchvision 0.13.1+cu116 requires torch==1.12.1, but you have torch 2.0.1 which is incompatible.\n",
      "torchaudio 0.12.1+cu116 requires torch==1.12.1, but you have torch 2.0.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed cmake-3.27.1 lit-16.0.6 mpmath-1.3.0 mypy-extensions-1.0.0 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 pyre-extensions-0.0.29 sympy-1.12 torch-2.0.1 triton-2.0.0 typing-inspect-0.9.0 xformers-0.0.20\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting lycoris_lora==0.1.4\n",
      "  Downloading lycoris_lora-0.1.4.tar.gz (37 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (from lycoris_lora==0.1.4) (2.0.1)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.9/dist-packages (from lycoris_lora==0.1.4) (0.2.6)\n",
      "Requirement already satisfied: diffusers in /usr/local/lib/python3.9/dist-packages (from lycoris_lora==0.1.4) (0.10.2)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (from lycoris_lora==0.1.4) (4.26.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from diffusers->lycoris_lora==0.1.4) (1.23.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from diffusers->lycoris_lora==0.1.4) (3.9.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from diffusers->lycoris_lora==0.1.4) (2022.10.31)\n",
      "Requirement already satisfied: huggingface-hub>=0.10.0 in /usr/local/lib/python3.9/dist-packages (from diffusers->lycoris_lora==0.1.4) (0.12.0)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.9/dist-packages (from diffusers->lycoris_lora==0.1.4) (9.2.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from diffusers->lycoris_lora==0.1.4) (2.28.2)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.9/dist-packages (from diffusers->lycoris_lora==0.1.4) (6.0.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch->lycoris_lora==0.1.4) (1.12)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.9/dist-packages (from torch->lycoris_lora==0.1.4) (11.10.3.66)\n",
      "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch->lycoris_lora==0.1.4) (2.0.0)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/lib/python3.9/dist-packages (from torch->lycoris_lora==0.1.4) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.9/dist-packages (from torch->lycoris_lora==0.1.4) (11.7.99)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch->lycoris_lora==0.1.4) (3.0)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/lib/python3.9/dist-packages (from torch->lycoris_lora==0.1.4) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.9/dist-packages (from torch->lycoris_lora==0.1.4) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.9/dist-packages (from torch->lycoris_lora==0.1.4) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.9/dist-packages (from torch->lycoris_lora==0.1.4) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/lib/python3.9/dist-packages (from torch->lycoris_lora==0.1.4) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/lib/python3.9/dist-packages (from torch->lycoris_lora==0.1.4) (11.7.91)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/lib/python3.9/dist-packages (from torch->lycoris_lora==0.1.4) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/lib/python3.9/dist-packages (from torch->lycoris_lora==0.1.4) (2.14.3)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch->lycoris_lora==0.1.4) (4.4.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch->lycoris_lora==0.1.4) (3.1.2)\n",
      "Requirement already satisfied: wheel in /usr/local/lib/python3.9/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch->lycoris_lora==0.1.4) (0.35.1)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch->lycoris_lora==0.1.4) (66.1.1)\n",
      "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch->lycoris_lora==0.1.4) (3.27.1)\n",
      "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch->lycoris_lora==0.1.4) (16.0.6)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers->lycoris_lora==0.1.4) (5.4.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers->lycoris_lora==0.1.4) (4.64.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers->lycoris_lora==0.1.4) (0.12.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers->lycoris_lora==0.1.4) (23.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata->diffusers->lycoris_lora==0.1.4) (3.11.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch->lycoris_lora==0.1.4) (2.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/dist-packages (from requests->diffusers->lycoris_lora==0.1.4) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->diffusers->lycoris_lora==0.1.4) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->diffusers->lycoris_lora==0.1.4) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->diffusers->lycoris_lora==0.1.4) (2019.11.28)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch->lycoris_lora==0.1.4) (1.3.0)\n",
      "Building wheels for collected packages: lycoris_lora\n",
      "  Building wheel for lycoris_lora (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for lycoris_lora: filename=lycoris_lora-0.1.4-py3-none-any.whl size=26133 sha256=b91b32685fbd2b8511b0e73edb477568adfc59385a6c35db55d941cc3814a2d0\n",
      "  Stored in directory: /root/.cache/pip/wheels/b5/89/21/900ccae8caa83dfe77dceba25e00d1711535f027fc671c9e61\n",
      "Successfully built lycoris_lora\n",
      "Installing collected packages: lycoris_lora\n",
      "Successfully installed lycoris_lora-0.1.4\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting dadaptation==1.5\n",
      "  Downloading dadaptation-1.5.tar.gz (8.3 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: dadaptation\n",
      "  Building wheel for dadaptation (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for dadaptation: filename=dadaptation-1.5-py3-none-any.whl size=14347 sha256=ed79a50a6f760ae8c4701e7d4f49cf10e125db77a2cab3cc160c29dd48e02548\n",
      "  Stored in directory: /root/.cache/pip/wheels/38/59/da/04b9a4f89e5a6dbb2f670fbd1d22dca5de1024d76da664656d\n",
      "Successfully built dadaptation\n",
      "Installing collected packages: dadaptation\n",
      "Successfully installed dadaptation-1.5\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting lion_pytorch==0.0.6\n",
      "  Downloading lion_pytorch-0.0.6-py3-none-any.whl (4.2 kB)\n",
      "Requirement already satisfied: torch>=1.6 in /usr/local/lib/python3.9/dist-packages (from lion_pytorch==0.0.6) (2.0.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.9/dist-packages (from torch>=1.6->lion_pytorch==0.0.6) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.9/dist-packages (from torch>=1.6->lion_pytorch==0.0.6) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.9/dist-packages (from torch>=1.6->lion_pytorch==0.0.6) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/lib/python3.9/dist-packages (from torch>=1.6->lion_pytorch==0.0.6) (11.7.91)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/lib/python3.9/dist-packages (from torch>=1.6->lion_pytorch==0.0.6) (11.7.101)\n",
      "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch>=1.6->lion_pytorch==0.0.6) (2.0.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.6->lion_pytorch==0.0.6) (4.4.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch>=1.6->lion_pytorch==0.0.6) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.9/dist-packages (from torch>=1.6->lion_pytorch==0.0.6) (10.9.0.58)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch>=1.6->lion_pytorch==0.0.6) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch>=1.6->lion_pytorch==0.0.6) (3.0)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/lib/python3.9/dist-packages (from torch>=1.6->lion_pytorch==0.0.6) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/lib/python3.9/dist-packages (from torch>=1.6->lion_pytorch==0.0.6) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.9/dist-packages (from torch>=1.6->lion_pytorch==0.0.6) (11.10.3.66)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch>=1.6->lion_pytorch==0.0.6) (3.9.0)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/lib/python3.9/dist-packages (from torch>=1.6->lion_pytorch==0.0.6) (2.14.3)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/lib/python3.9/dist-packages (from torch>=1.6->lion_pytorch==0.0.6) (11.4.0.1)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.6->lion_pytorch==0.0.6) (66.1.1)\n",
      "Requirement already satisfied: wheel in /usr/local/lib/python3.9/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.6->lion_pytorch==0.0.6) (0.35.1)\n",
      "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.6->lion_pytorch==0.0.6) (16.0.6)\n",
      "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.6->lion_pytorch==0.0.6) (3.27.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch>=1.6->lion_pytorch==0.0.6) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch>=1.6->lion_pytorch==0.0.6) (1.3.0)\n",
      "Installing collected packages: lion_pytorch\n",
      "Successfully installed lion_pytorch-0.0.6\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gradient 2.0.6 requires attrs<=19, but you have attrs 23.1.0 which is incompatible.\n",
      "gradient-utils 0.5.0 requires pymongo<4.0.0,>=3.11.0, but you have pymongo 4.4.1 which is incompatible.\n",
      "gql 3.0.0a6 requires graphql-core<3.2,>=3.1.5, but you have graphql-core 3.2.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "#import os\n",
    "#os.chdir(\"/notebooks\")\n",
    "\n",
    "!pwd\n",
    "!cd /notebooks/\n",
    "!pip install accelerate==0.15.0\n",
    "!pip install transformers==4.26.0\n",
    "!pip install ftfy==6.1.1\n",
    "!pip install albumentations==1.3.0\n",
    "!pip install opencv-python==4.7.0.68\n",
    "!pip install einops==0.6.0\n",
    "!pip install diffusers==0.10.2\n",
    "!pip install pytorch-lightning==1.9.0\n",
    "!pip install bitsandbytes==0.35.0\n",
    "!pip install tensorflow==2.11.0\n",
    "!pip install safetensors==0.2.6\n",
    "!pip install toml==0.10.2\n",
    "!pip install voluptuous==0.13.1\n",
    "!pip install xformers==0.0.20\n",
    "!pip install lycoris_lora==0.1.4\n",
    "!pip install dadaptation==1.5\n",
    "!pip install lion_pytorch==0.0.6\n",
    "!pip -q install fiftyone ftfy\n",
    "!pip -q install fiftyone-db-ubuntu2004\n",
    "\n",
    "\n",
    "#!ls\n",
    "#!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *️⃣ Copy Images & Create Folder Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-10T18:16:35.726162Z",
     "iopub.status.busy": "2023-08-10T18:16:35.725714Z",
     "iopub.status.idle": "2023-08-10T18:16:40.404308Z",
     "shell.execute_reply": "2023-08-10T18:16:40.403242Z",
     "shell.execute_reply.started": "2023-08-10T18:16:35.726117Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: torchvision\n",
      "Version: 0.13.1+cu116\n",
      "Summary: image and video datasets and models for torch deep learning\n",
      "Home-page: https://github.com/pytorch/vision\n",
      "Author: PyTorch Core Team\n",
      "Author-email: soumith@pytorch.org\n",
      "License: BSD\n",
      "Location: /usr/local/lib/python3.9/dist-packages\n",
      "Requires: numpy, pillow, requests, torch, typing-extensions\n",
      "Required-by: sentence-transformers\n"
     ]
    }
   ],
   "source": [
    "my_project_name=\"donabull\"\n",
    "!pip show torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "cellView": "form",
    "execution": {
     "iopub.execute_input": "2023-08-10T18:32:23.248778Z",
     "iopub.status.busy": "2023-08-10T18:32:23.248264Z",
     "iopub.status.idle": "2023-08-10T18:32:24.368390Z",
     "shell.execute_reply": "2023-08-10T18:32:24.366778Z",
     "shell.execute_reply.started": "2023-08-10T18:32:23.248737Z"
    },
    "id": "cBa7KdewQ4BU",
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delet old images if exists\n",
      "Root Dirctory for donabull is /storage/loras!\n",
      "Copy all files from! /datasets/donabull to /storage/loras/donabull/dataset\n",
      "✅ Project donabull is ready!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from IPython import get_ipython\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "COLAB = False\n",
    "GRAD = False\n",
    "\n",
    "if COLAB:\n",
    "  from google.colab.output import clear as clear_output\n",
    "else:\n",
    "  from IPython.display import clear_output\n",
    "\n",
    "\n",
    "#@title ## 🚩 Start Here\n",
    "\n",
    "#@markdown ### 1️⃣ Setup\n",
    "#@markdown This cell will load some requirements and create the necessary folders in your Google Drive. <p>\n",
    "#@markdown Your project name can't contain spaces but it can contain a single / to make a subfolder in your dataset.\n",
    "project_name = my_project_name #@param {type:\"string\"}\n",
    "project_name = project_name.strip()\n",
    "#@markdown The folder structure doesn't matter and is purely for comfort. Make sure to always pick the same one. I like organizing by project.\n",
    "folder_structure = \"Organize by project (MyDrive/Loras/project_name/dataset)\" #@param [\"Organize by category (MyDrive/lora_training/datasets/project_name)\", \"Organize by project (MyDrive/Loras/project_name/dataset)\"]\n",
    "\n",
    "if not project_name or any(c in project_name for c in \" .()\\\"'\\\\\") or project_name.count(\"/\") > 1:\n",
    "  print(\"Please write a valid project_name.\")\n",
    "else:\n",
    "  if COLAB and not os.path.exists('/content/drive'):\n",
    "    from google.colab import drive\n",
    "    print(\"📂 Connecting to Google Drive...\")\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "  project_base = project_name if \"/\" not in project_name else project_name[:project_name.rfind(\"/\")]\n",
    "  project_subfolder = project_name if \"/\" not in project_name else project_name[project_name.rfind(\"/\")+1:]\n",
    "\n",
    "  root_dir = \"/storage/loras\"\n",
    "  deps_dir = os.path.join(root_dir, \"deps\")\n",
    "  main_dir      =  root_dir\n",
    "  config_folder = os.path.join(main_dir, project_base)\n",
    "  images_folder = os.path.join(main_dir, project_base, \"dataset\")\n",
    "  if \"/\" in project_name:\n",
    "        images_folder = os.path.join(images_folder, project_subfolder)\n",
    "\n",
    "\n",
    "  print(f\"delet old images if exists\")\n",
    "  shutil.rmtree(images_folder,ignore_errors=True)\n",
    "\n",
    "    \n",
    "  print(f\"Root Dirctory for {project_name} is {root_dir}!\")\n",
    "  for dir in [main_dir, deps_dir, images_folder, config_folder]:\n",
    "    #print(f\" Making {dir}\")\n",
    "    os.makedirs(dir, exist_ok=True)\n",
    "\n",
    "  print(f\"Copy all files from! /datasets/\"+project_base +\" to \"+ images_folder)  \n",
    "  shutil.copytree(\"/datasets/\"+project_base +\"/\", images_folder+\"/\",dirs_exist_ok = True)\n",
    "\n",
    "  print(f\"✅ Project {project_name} is ready!\")\n",
    "  step1_installed_flag = True\n",
    "    \n",
    "\n",
    "#@markdown ### 🚮 Clean folder\n",
    "#@markdown Careful! Deletes all non-image files in the project folder.\n",
    "\n",
    "!find {images_folder} -type f ! \\( -name '*.png' -o -name '*.jpg' -o -name '*.jpeg' \\) -delete \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *️⃣ Image Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "cellView": "form",
    "execution": {
     "iopub.execute_input": "2023-08-10T18:32:47.177244Z",
     "iopub.status.busy": "2023-08-10T18:32:47.176824Z",
     "iopub.status.idle": "2023-08-10T18:33:10.158585Z",
     "shell.execute_reply": "2023-08-10T18:33:10.157221Z",
     "shell.execute_reply.started": "2023-08-10T18:32:47.177210Z"
    },
    "id": "b218DEEMpwzB",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Removed 2 images from dataset. You now have 183 images.\n"
     ]
    }
   ],
   "source": [
    "if \"step1_installed_flag\" not in globals():\n",
    "  raise Exception(\"Please run step 1 first!\")\n",
    "\n",
    "#@markdown ### 3️⃣ Curate your images\n",
    "#@markdown We will find duplicate images with the FiftyOne AI, and mark them with `delete`. <p>\n",
    "#@markdown Then, an interactive area will appear below this cell that lets you visualize all your images and manually mark with `delete` to the ones you don't like. <p>\n",
    "#@markdown If the interactive area appears blank for over a minute, try enabling cookies and removing tracking protection for the Google Colab website, as they may break it.\n",
    "#@markdown Regardless, you can save your changes by sending Enter in the input box above the interactive area.<p>\n",
    "#@markdown This is how similar 2 images must be to be marked for deletion. I recommend 0.97 to 0.99:\n",
    "similarity_threshold = 0.985 #@param {type:\"number\"}\n",
    "\n",
    "print(\"root_dir \"+root_dir)\n",
    "print(\"images_folder \"+images_folder)\n",
    "os.chdir(root_dir)\n",
    "model_name = \"clip-vit-base32-torch\"\n",
    "supported_types = (\".png\", \".jpg\", \".jpeg\")\n",
    "img_count = len(os.listdir(images_folder))\n",
    "batch_size = min(250, img_count)\n",
    "\n",
    "if \"step3_installed_flag\" not in globals():\n",
    "  print(\"🏭 Installing dependencies...\\n\")\n",
    "  !pip -q install fiftyone ftfy\n",
    "  !pip -q install fiftyone-db-ubuntu2004\n",
    "  if not get_ipython().__dict__['user_ns']['_exit_code']:\n",
    "    clear_output()\n",
    "    step3_installed_flag = True\n",
    "  else:\n",
    "    print(\"❌ Error installing dependencies, attempting to continue anyway...\")\n",
    "\n",
    "import numpy as np\n",
    "import fiftyone as fo\n",
    "import fiftyone.zoo as foz\n",
    "from fiftyone import ViewField as F\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "non_images = [f for f in os.listdir(images_folder) if not f.lower().endswith(supported_types)]\n",
    "if non_images:\n",
    "  print(f\"💥 Error: Found non-image file {non_images[0]} - This program doesn't allow it. Sorry! Use the Extras at the bottom to clean the folder.\")\n",
    "elif img_count == 0:\n",
    "  print(f\"💥 Error: No images found in {images_folder}\")\n",
    "else:\n",
    "  print(\"\\n💿 Analyzing dataset...\\n\")\n",
    "  dataset = fo.Dataset.from_dir(images_folder, dataset_type=fo.types.ImageDirectory)\n",
    "  model = foz.load_zoo_model(model_name)\n",
    "  embeddings = dataset.compute_embeddings(model, batch_size=batch_size)\n",
    "\n",
    "  batch_embeddings = np.array_split(embeddings, batch_size)\n",
    "  similarity_matrices = []\n",
    "  max_size_x = max(array.shape[0] for array in batch_embeddings)\n",
    "  max_size_y = max(array.shape[1] for array in batch_embeddings)\n",
    "\n",
    "  for i, batch_embedding in enumerate(batch_embeddings):\n",
    "    similarity = cosine_similarity(batch_embedding)\n",
    "    #Pad 0 for np.concatenate\n",
    "    padded_array = np.zeros((max_size_x, max_size_y))\n",
    "    padded_array[0:similarity.shape[0], 0:similarity.shape[1]] = similarity\n",
    "    similarity_matrices.append(padded_array)\n",
    "\n",
    "  similarity_matrix = np.concatenate(similarity_matrices, axis=0)\n",
    "  similarity_matrix = similarity_matrix[0:embeddings.shape[0], 0:embeddings.shape[0]]\n",
    "\n",
    "  similarity_matrix = cosine_similarity(embeddings)\n",
    "  similarity_matrix -= np.identity(len(similarity_matrix))\n",
    "\n",
    "  dataset.match(F(\"max_similarity\") > similarity_threshold)\n",
    "  dataset.tags = [\"delete\", \"has_duplicates\"]\n",
    "\n",
    "  id_map = [s.id for s in dataset.select_fields([\"id\"])]\n",
    "  samples_to_remove = set()\n",
    "  samples_to_keep = set()\n",
    "\n",
    "  for idx, sample in enumerate(dataset):\n",
    "    if sample.id not in samples_to_remove:\n",
    "      # Keep the first instance of two duplicates\n",
    "      samples_to_keep.add(sample.id)\n",
    "      \n",
    "      dup_idxs = np.where(similarity_matrix[idx] > similarity_threshold)[0]\n",
    "      for dup in dup_idxs:\n",
    "          # We kept the first instance so remove all other duplicates\n",
    "          samples_to_remove.add(id_map[dup])\n",
    "\n",
    "      if len(dup_idxs) > 0:\n",
    "          sample.tags.append(\"has_duplicates\")\n",
    "          sample.save()\n",
    "    else:\n",
    "      sample.tags.append(\"delete\")\n",
    "      sample.save()\n",
    "\n",
    "  clear_output()\n",
    "\n",
    "  sidebar_groups = fo.DatasetAppConfig.default_sidebar_groups(dataset)\n",
    "  for group in sidebar_groups[1:]:\n",
    "    group.expanded = False\n",
    "  dataset.app_config.sidebar_groups = sidebar_groups\n",
    "  dataset.save()\n",
    "  session = fo.launch_app(dataset)\n",
    "\n",
    "  print(\"❗ Wait a minute for the session to load. If it doesn't, read above.\")\n",
    "  print(\"❗ When it's ready, you'll see a grid of your images.\")\n",
    "  print(\"❗ On the left side enable \\\"sample tags\\\" to visualize the images marked for deletion.\")\n",
    "  print(\"❗ You can mark your own images with the \\\"delete\\\" label by selecting them and pressing the tag icon at the top.\")\n",
    "  input(\"⭕ When you're done, enter something here to save your changes: \")\n",
    "\n",
    "  print(\"💾 Saving...\")\n",
    "\n",
    "  kys = [s for s in dataset if \"delete\" in s.tags]\n",
    "  dataset.remove_samples(kys)\n",
    "  previous_folder = images_folder[:images_folder.rfind(\"/\")]\n",
    "  dataset.export(export_dir=os.path.join(images_folder, project_subfolder), dataset_type=fo.types.ImageDirectory)\n",
    "  \n",
    "  temp_suffix = \"_temp\"\n",
    "  !mv {images_folder} {images_folder}{temp_suffix}\n",
    "  !mv {images_folder}{temp_suffix}/{project_subfolder} {images_folder}\n",
    "  !rm -r {images_folder}{temp_suffix}\n",
    "\n",
    "  session.refresh()\n",
    "  fo.close_app()\n",
    "  clear_output()\n",
    "\n",
    "  print(f\"\\n✅ Removed {len(kys)} images from dataset. You now have {len(os.listdir(images_folder))} images.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *️⃣ Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-10T18:40:38.642508Z",
     "iopub.status.busy": "2023-08-10T18:40:38.641519Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/storage/loras\n",
      "images_folder /storage/loras/donabull/dataset\n",
      "2023-08-10 18:40:39.955911: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-10 18:40:40.227440: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-08-10 18:40:41.318381: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /storage/env/tagger/lib/python3.9/site-packages/cv2/../../lib64:/usr/local/lib/python3.9/dist-packages/cv2/../../lib64:/usr/local/cuda-11.6/lib64\n",
      "2023-08-10 18:40:41.318508: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /storage/env/tagger/lib/python3.9/site-packages/cv2/../../lib64:/usr/local/lib/python3.9/dist-packages/cv2/../../lib64:/usr/local/cuda-11.6/lib64\n",
      "2023-08-10 18:40:41.318532: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "using existing wd14 tagger model\n",
      "found 183 images.\n",
      "loading model and labels\n",
      "2023-08-10 18:40:46.918092: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-10 18:40:46.920949: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-10 18:40:46.921248: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-10 18:40:46.921863: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-10 18:40:46.922970: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-10 18:40:46.923264: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-10 18:40:46.923516: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-10 18:40:49.291221: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-10 18:40:49.291541: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-10 18:40:49.291870: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-10 18:40:49.292104: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 12978 MB memory:  -> device: 0, name: Quadro P5000, pci bus id: 0000:00:05.0, compute capability: 6.1\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "  3%|█▍                                         | 6/183 [00:00<00:06, 26.64it/s]2023-08-10 18:41:22.072992: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8500\n",
      "100%|█████████████████████████████████████████| 183/183 [00:27<00:00,  6.65it/s]\n",
      "done!\n",
      "2023-08-10 18:41:56.191281: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-10 18:41:56.476428: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-08-10 18:41:57.383253: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib/python3.9/dist-packages/cv2/../../lib64:/usr/local/cuda-11.6/lib64\n",
      "2023-08-10 18:41:57.383373: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib/python3.9/dist-packages/cv2/../../lib64:/usr/local/cuda-11.6/lib64\n",
      "2023-08-10 18:41:57.383396: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "Current Working Directory is:  /storage/content/kohya-trainer\n",
      "load images from /storage/loras/donabull/dataset\n",
      "found 183 images.\n",
      "loading BLIP caption: https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_large_caption.pth\n",
      "load checkpoint from https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_large_caption.pth\n",
      "BLIP loaded\n",
      " 87%|█████████████████████████████████████▍     | 20/23 [00:41<00:06,  2.08s/it]"
     ]
    }
   ],
   "source": [
    "!pwd\n",
    "!bash /notebooks/tagger.sh {project_name}\n",
    "!find {images_folder} -type f ! \\( -name '*.png' -o -name '*.jpg' -o -name '*.jpeg' -o -name '*.txt'  -o -name '*.JPG'  -o -name '*.PNG'  -o -name '*.JPEG'   \\) -delete\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2023-08-09T20:46:35.753190Z",
     "iopub.status.busy": "2023-08-09T20:46:35.752141Z",
     "iopub.status.idle": "2023-08-09T20:46:36.206484Z",
     "shell.execute_reply": "2023-08-09T20:46:36.204452Z",
     "shell.execute_reply.started": "2023-08-09T20:46:35.753177Z"
    },
    "id": "sl4FD7Mz-uea",
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚶‍♂️ Launching program...\n",
      "\n",
      "env: PYTHONPATH=/content/kohya-trainer\n",
      "Traceback (most recent call last):\n",
      "  File \"/content/kohya-trainer/finetune/tag_images_by_wd14_tagger.py\", line 14, in <module>\n",
      "    import library.train_util as train_util\n",
      "  File \"/content/kohya-trainer/library/train_util.py\", line 21, in <module>\n",
      "    from accelerate import Accelerator\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/accelerate/__init__.py\", line 7, in <module>\n",
      "    from .accelerator import Accelerator\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/accelerate/accelerator.py\", line 27, in <module>\n",
      "    from .checkpointing import load_accelerator_state, load_custom_state, save_accelerator_state, save_custom_state\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/accelerate/checkpointing.py\", line 22, in <module>\n",
      "    from torch.cuda.amp import GradScaler\n",
      "ModuleNotFoundError: No module named 'torch.cuda'\n"
     ]
    }
   ],
   "source": [
    "if \"step1_installed_flag\" not in globals():\n",
    "  raise Exception(\"Please run step 1 first!\")\n",
    "\n",
    "#@markdown ### 4️⃣ Tag your images\n",
    "#@markdown We will be using AI to automatically tag your images, specifically [Waifu Diffusion](https://huggingface.co/SmilingWolf/wd-v1-4-swinv2-tagger-v2) in the case of anime and [BLIP](https://huggingface.co/spaces/Salesforce/BLIP) in the case of photos.\n",
    "#@markdown Giving tags/captions to your images allows for much better training. This process should take a couple minutes. <p>\n",
    "method = \"Photo captions\" #@param [\"Anime tags\", \"Photo captions\"]\n",
    "is_tag_enabled = True #@param [\"Anime tags\", \"Photo captions\"]\n",
    "is_caption_enabled = True\n",
    "#@markdown **Anime:** The threshold is the minimum level of confidence the tagger must have in order to include a tag. Lower threshold = More tags. Recommended 0.35 to 0.5\n",
    "tag_threshold = 0.35 #@param {type:\"slider\", min:0.0, max:1.0, step:0.01}\n",
    "blacklist_tags = \"bangs, multicolored hair, two-tone hair, gradient hair, virtual youtuber, official alternate costume, official alternate hairstyle, official alternate hair length, alternate costume, alternate hairstyle, alternate hair length, alternate hair color\" #@param {type:\"string\"}\n",
    "#@markdown **Photos:** The minimum and maximum length of tokens/words in each caption.\n",
    "caption_min = 10 #@param {type:\"number\"}\n",
    "caption_max = 75 #@param {type:\"number\"}\n",
    "\n",
    "%env PYTHONPATH=/env/python\n",
    "os.chdir(root_dir)\n",
    "kohya = \"/storage/content/kohya-trainer\"\n",
    "if not os.path.exists(kohya):\n",
    "  !git clone https://github.com/kohya-ss/sd-scripts {kohya}\n",
    "  os.chdir(kohya)\n",
    "  !git reset --hard 5050971ac687dca70ba0486a583d283e8ae324e2\n",
    "  os.chdir(root_dir)  \n",
    "    \n",
    "\n",
    "if is_tag_enabled == True:\n",
    "    print(\"\\n🏭 Installing dependencies...\\n\")\n",
    "    !pip uninstall -y torch\n",
    "    !pip install torch==1.12.0\n",
    "    !pip install tensorflow==2.11.0 \n",
    "    !pip install huggingface-hub==0.12.0 \n",
    "    !pip install accelerate==0.15.0 \n",
    "    !pip install transformers==4.26.0 \n",
    "    !pip install diffusers[torch]==0.10.2 \n",
    "    !pip install einops==0.6.0 \n",
    "    !pip install safetensors==0.2.6\n",
    "    !pip install torchvision\n",
    "    !pip install albumentations\n",
    "    #!pip -q install tensorflow==2.11.0 huggingface-hub==0.12.0 accelerate==0.15.0 transformers==4.26.0 diffusers[torch]==0.10.2 einops==0.6.0 safetensors==0.2.6 torchvision albumentations\n",
    "    if not get_ipython().__dict__['user_ns']['_exit_code']:\n",
    "      clear_output()\n",
    "      step4a_installed_flag = True\n",
    "    else:\n",
    "      print(\"❌ Error installing dependencies, trying to continue anyway...\")\n",
    "\n",
    "    print(\"\\n🚶‍♂️ Launching program...\\n\")\n",
    "\n",
    "    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "    %env PYTHONPATH={kohya}\n",
    "    !python {kohya}/finetune/tag_images_by_wd14_tagger.py \\\n",
    "        {images_folder} \\\n",
    "        --repo_id=SmilingWolf/wd-v1-4-swinv2-tagger-v2 \\\n",
    "        --model_dir={root_dir} \\\n",
    "        --thresh={tag_threshold} \\\n",
    "        --batch_size=8 \\\n",
    "        --caption_extension=.tags \\\n",
    "        --force_download\n",
    "\n",
    "    if not get_ipython().__dict__['user_ns']['_exit_code']:\n",
    "        print(\"removing underscores and blacklist...\")\n",
    "        blacklisted_tags = [t.strip() for t in blacklist_tags.split(\",\")]\n",
    "        from collections import Counter\n",
    "        top_tags = Counter()\n",
    "        for txt in [f for f in os.listdir(images_folder) if f.lower().endswith(\".txt\")]:\n",
    "          with open(os.path.join(images_folder, txt), 'r') as f:\n",
    "            tags = [t.strip() for t in f.read().split(\",\")]\n",
    "            tags = [t.replace(\"_\", \" \") if len(t) > 3 else t for t in tags]\n",
    "            tags = [t for t in tags if t not in blacklisted_tags]\n",
    "          top_tags.update(tags)\n",
    "          with open(os.path.join(images_folder, txt), 'w') as f:\n",
    "            f.write(\", \".join(tags))\n",
    "\n",
    "        %env PYTHONPATH=/env/python\n",
    "        clear_output()\n",
    "        print(f\"📊 Tagging complete. Here are the top 50 tags in your dataset:\")\n",
    "        print(\"\\n\".join(f\"{k} ({v})\" for k, v in top_tags.most_common(50)))\n",
    "        \n",
    "        \n",
    "if is_caption_enabled == True: # Photos\n",
    "  if \"step4a_installed_flag\" not in globals():\n",
    "    print(\"\\n🏭 Installing dependencies...\\n\")\n",
    "    !pip uninstall -y torch\n",
    "    !pip install torch==1.12.0 \n",
    "    !pip install timm==0.6.12 \n",
    "    !pip install fairscale==0.4.13 \n",
    "    !pip install transformers==4.26.0 \n",
    "    !pip install requests==2.28.2 \n",
    "    !pip install accelerate==0.15.0 \n",
    "    !pip install diffusers[torch]==0.10.2 \n",
    "    !pip install einops==0.6.0 \n",
    "    !pip install safetensors==0.2.6\n",
    "    #!pip -q install timm==0.6.12 fairscale==0.4.13 transformers==4.26.0 requests==2.28.2 accelerate==0.15.0 diffusers[torch]==0.10.2 einops==0.6.0 safetensors==0.2.6\n",
    "    if not get_ipython().__dict__['user_ns']['_exit_code']:\n",
    "      clear_output()\n",
    "      step4a_installed_flag = True\n",
    "    else:\n",
    "      print(\"❌ Error installing dependencies, trying to continue anyway...\")\n",
    "\n",
    "  print(\"\\n🚶‍♂️ Launching program...\\n\")\n",
    "  os.chdir(kohya)\n",
    "  %env PYTHONPATH={kohya}\n",
    "  !python {kohya}/finetune/make_captions.py \\\n",
    "    {images_folder} \\\n",
    "    --beam_search \\\n",
    "    --max_data_loader_n_workers=2 \\\n",
    "    --batch_size=8 \\\n",
    "    --min_length={caption_min} \\\n",
    "    --max_length={caption_max} \\\n",
    "    --caption_extension=.caption\n",
    "\n",
    "  if not get_ipython().__dict__['user_ns']['_exit_code']:\n",
    "    import random\n",
    "    captions = [f for f in os.listdir(images_folder) if f.lower().endswith(\".txt\")]\n",
    "    sample = []\n",
    "    for txt in random.sample(captions, min(10, len(captions))):\n",
    "      with open(os.path.join(images_folder, txt), 'r') as f:\n",
    "        sample.append(f.read())\n",
    "\n",
    "    os.chdir(root_dir)\n",
    "    %env PYTHONPATH=/env/python\n",
    "    clear_output()\n",
    "    print(f\"📊 Captioning complete. Here are {len(sample)} example captions from your dataset:\")\n",
    "    print(\"\".join(sample))\n",
    "\n",
    "    print(f\"train_data_dir = {args.train_data_dir}\")\n",
    "    train_data_dir_path = Path(args.train_data_dir)\n",
    "    print(f\"train_data_dir_path= {train_data_dir_path}\")\n",
    "\n",
    "# fix the seed for reproducibility\n",
    "for txt in [f for f in os.listdir(images_folder) if f.lower().endswith(\".caption\")]:\n",
    "  image_name,image_ext =  os.path.splitext(txt)\n",
    "  #print(f\"image_path={os.path.join(images_folder,image_name + \".caption\")}\")\n",
    "  tags=\"\"\n",
    "  caption=\"\"\n",
    "\n",
    "  with open(os.path.join(images_folder,image_name + args.caption_extension), \"r\") as f:\n",
    "    caption=f.read().strip() \n",
    "  with open(os.path.join(images_folder, image_name + args.tags_extention), \"r\") as f:\n",
    "    tags = f.read().strip() \n",
    "  with open(os.path.join(images_folder, image_name + args.txt_extention), \"wt\", encoding=\"utf-8\") as f:\n",
    "    print(f\"{caption}\")\n",
    "    f.write(caption + \", \" + tags)\n",
    "\n",
    "!find {images_folder} -type f ! \\( -name '*.png' -o -name '*.jpg' -o -name '*.jpeg' -o -name '*.txt'  -o -name '*.JPG'  -o -name '*.PNG'  -o -name '*.JPEG'   \\) -delete\n",
    "\n",
    "print(\"done!\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *️⃣ Global Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "cellView": "form",
    "execution": {
     "iopub.execute_input": "2023-08-10T18:38:00.455254Z",
     "iopub.status.busy": "2023-08-10T18:38:00.454778Z",
     "iopub.status.idle": "2023-08-10T18:38:01.549501Z",
     "shell.execute_reply": "2023-08-10T18:38:01.548500Z",
     "shell.execute_reply.started": "2023-08-10T18:38:00.455208Z"
    },
    "id": "WBFik7accyDz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove '.ipynb_checkpoints': No such file or directory\n",
      "\n",
      "📎 Applied new activation tag(s): donabull\n",
      "\n",
      "✅ Done! Check your updated tags in the Extras below.\n"
     ]
    }
   ],
   "source": [
    "!find {images_folder} -type f ! \\( -name '*.png' -o -name '*.jpg' -o -name '*.jpeg' \\) -delete\n",
    "!rm -r .ipynb_checkpoints\n",
    "\n",
    "\n",
    "if \"step1_installed_flag\" not in globals():\n",
    "  raise Exception(\"Please run step 1 first!\")\n",
    "  \n",
    "#@markdown ### 5️⃣ Curate your tags\n",
    "#@markdown Modify your dataset's tags. You can run this cell multiple times with different parameters. <p>\n",
    "\n",
    "#@markdown Put an activation tag at the start of every text file. This is useful to make learning better and activate your Lora easier. Set `keep_tokens` to 1 when training.<p>\n",
    "#@markdown Common tags that are removed such as hair color, etc. will be \"absorbed\" by your activation tag.\n",
    "global_activation_tag = project_name #@param {type:\"string\"}\n",
    "remove_tags = \"\" #@param {type:\"string\"}\n",
    "#@markdown &nbsp;\n",
    "\n",
    "#@markdown In this advanced section, you can search text files containing matching tags, and replace them with less/more/different tags. If you select the checkbox below, any extra tags will be put at the start of the file, letting you assign different activation tags to different parts of your dataset. Still, you may want a more advanced tool for this.\n",
    "search_tags = \"\" #@param {type:\"string\"}\n",
    "replace_with = \"\" #@param {type:\"string\"}\n",
    "search_mode = \"OR\" #@param [\"OR\", \"AND\"]\n",
    "new_becomes_activation_tag = False #@param {type:\"boolean\"}\n",
    "#@markdown These may be useful sometimes. Will remove existing activation tags, be careful.\n",
    "sort_alphabetically = False #@param {type:\"boolean\"}\n",
    "remove_duplicates = False #@param {type:\"boolean\"}\n",
    "\n",
    "def split_tags(tagstr):\n",
    "  return [s.strip() for s in tagstr.split(\",\") if s.strip()]\n",
    "\n",
    "activation_tag_list = split_tags(global_activation_tag)\n",
    "remove_tags_list = split_tags(remove_tags)\n",
    "search_tags_list = split_tags(search_tags)\n",
    "replace_with_list = split_tags(replace_with)\n",
    "replace_new_list = [t for t in replace_with_list if t not in search_tags_list]\n",
    "\n",
    "replace_with_list = [t for t in replace_with_list if t not in replace_new_list]\n",
    "replace_new_list.reverse()\n",
    "activation_tag_list.reverse()\n",
    "\n",
    "remove_count = 0\n",
    "replace_count = 0\n",
    "\n",
    "for txt in [f for f in os.listdir(images_folder) if f.lower().endswith(\".txt\")]:\n",
    "\n",
    "  with open(os.path.join(images_folder, txt), 'r') as f:\n",
    "    tags = [s.strip() for s in f.read().split(\",\")]\n",
    "\n",
    "  if remove_duplicates:\n",
    "    tags = list(set(tags))\n",
    "  if sort_alphabetically:\n",
    "    tags.sort()\n",
    "\n",
    "  for rem in remove_tags_list:\n",
    "    if rem in tags:\n",
    "      remove_count += 1\n",
    "      tags.remove(rem)\n",
    "\n",
    "  if \"AND\" in search_mode and all(r in tags for r in search_tags_list) \\\n",
    "      or \"OR\" in search_mode and any(r in tags for r in search_tags_list):\n",
    "    replace_count += 1\n",
    "    for rem in search_tags_list:\n",
    "      if rem in tags:\n",
    "        tags.remove(rem)\n",
    "    for add in replace_with_list:\n",
    "      if add not in tags:\n",
    "        tags.append(add)\n",
    "    for new in replace_new_list:\n",
    "      if new_becomes_activation_tag:\n",
    "        if new in tags:\n",
    "          tags.remove(new)\n",
    "        tags.insert(0, new)\n",
    "      else:\n",
    "        if new not in tags:\n",
    "          tags.append(new)\n",
    "\n",
    "  for act in activation_tag_list:\n",
    "    if act in tags:\n",
    "      tags.remove(act)\n",
    "    tags.insert(0, act)\n",
    "\n",
    "  with open(os.path.join(images_folder, txt), 'w') as f:\n",
    "    f.write(\", \".join(tags))\n",
    "\n",
    "if global_activation_tag:\n",
    "  print(f\"\\n📎 Applied new activation tag(s): {', '.join(activation_tag_list)}\")\n",
    "if remove_tags:\n",
    "  print(f\"\\n🚮 Removed {remove_count} tags.\")\n",
    "if search_tags:\n",
    "  print(f\"\\n💫 Replaced in {replace_count} files.\")\n",
    "print(\"\\n✅ Done! Check your updated tags in the Extras below.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gDB9GXRONfiU"
   },
   "source": [
    "## *️⃣ Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-09T22:34:18.121974Z",
     "iopub.status.busy": "2023-08-09T22:34:18.121192Z",
     "iopub.status.idle": "2023-08-09T22:34:18.129316Z",
     "shell.execute_reply": "2023-08-09T22:34:18.127988Z",
     "shell.execute_reply.started": "2023-08-09T22:34:18.121945Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For 47.0 files 4700.0\n"
     ]
    }
   ],
   "source": [
    "# ImageCount x Repeat x Epoch < 20000\n",
    "my_image_count=len(os.listdir(images_folder))/2\n",
    "my_per_image_repeat=5\n",
    "my_net_epoch=20\n",
    "print(f\"For {my_image_count} files\",str(my_image_count*my_per_image_repeat*my_net_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-09T22:34:20.280137Z",
     "iopub.status.busy": "2023-08-09T22:34:20.279760Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "project_name desipose\n",
      "folder_structure Organize by project (MyDrive/Loras/project_name/dataset)\n",
      "main_dir /storage/loras\n",
      "config_folder /storage/loras/desipose\n",
      "images_folder /storage/loras/desipose/dataset\n",
      "project_name desipose\n",
      "folder_structure Organize by project (MyDrive/Loras/project_name/dataset)\n",
      "main_dir /storage/loras\n",
      "config_folder /storage/loras/desipose\n",
      "images_folder /storage/loras/desipose/dataset\n",
      "output_folder /storage/loras/desipose/output\n",
      "\n",
      "💿 Checking dataset...\n",
      "📁/storage/loras/desipose/dataset\n",
      "📈 Found 94 images with 5 repeats, equaling 470 steps.\n",
      "📉 Divide 470 steps by 2 batch size to get 235.0 steps per epoch.\n",
      "🔮 There will be 20 epochs, for around 4700 total training steps.\n",
      "\n",
      "🏭 Installing dependencies...\n",
      "\n",
      "fatal: destination path '/storage/loras/kohya-trainer' already exists and is not an empty directory.\n",
      "HEAD is now at e6ad3cb Merge pull request #478 from rockerBOO/patch-1\n",
      "143 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
      "The following additional packages will be installed:\n",
      "  libaria2-0 libc-ares2 libssh2-1\n",
      "The following NEW packages will be installed:\n",
      "  aria2 libaria2-0 libc-ares2 libssh2-1\n",
      "0 upgraded, 4 newly installed, 0 to remove and 143 not upgraded.\n",
      "Need to get 1551 kB of archives.\n",
      "After this operation, 6235 kB of additional disk space will be used.\n",
      "\n",
      "\u001b7\u001b[0;23r\u001b8\u001b[1ASelecting previously unselected package libc-ares2:amd64.\n",
      "(Reading database ... 69943 files and directories currently installed.)\n",
      "Preparing to unpack .../libc-ares2_1.15.0-1ubuntu0.3_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  0%]\u001b[49m\u001b[39m [..........................................................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  6%]\u001b[49m\u001b[39m [###.......................................................] \u001b8Unpacking libc-ares2:amd64 (1.15.0-1ubuntu0.3) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 12%]\u001b[49m\u001b[39m [######....................................................] \u001b8Selecting previously unselected package libssh2-1:amd64.\n",
      "Preparing to unpack .../libssh2-1_1.8.0-2.1build1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 18%]\u001b[49m\u001b[39m [##########................................................] \u001b8Unpacking libssh2-1:amd64 (1.8.0-2.1build1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 24%]\u001b[49m\u001b[39m [#############.............................................] \u001b8Selecting previously unselected package libaria2-0:amd64.\n",
      "Preparing to unpack .../libaria2-0_1.35.0-1build1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 29%]\u001b[49m\u001b[39m [#################.........................................] \u001b8Unpacking libaria2-0:amd64 (1.35.0-1build1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 35%]\u001b[49m\u001b[39m [####################......................................] \u001b8Selecting previously unselected package aria2.\n",
      "Preparing to unpack .../aria2_1.35.0-1build1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 41%]\u001b[49m\u001b[39m [#######################...................................] \u001b8Unpacking aria2 (1.35.0-1build1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 47%]\u001b[49m\u001b[39m [###########################...............................] \u001b8Setting up libc-ares2:amd64 (1.15.0-1ubuntu0.3) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 53%]\u001b[49m\u001b[39m [##############################............................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 59%]\u001b[49m\u001b[39m [##################################........................] \u001b8Setting up libssh2-1:amd64 (1.8.0-2.1build1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 65%]\u001b[49m\u001b[39m [#####################################.....................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 71%]\u001b[49m\u001b[39m [########################################..................] \u001b8Setting up libaria2-0:amd64 (1.35.0-1build1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 76%]\u001b[49m\u001b[39m [############################################..............] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 82%]\u001b[49m\u001b[39m [###############################################...........] \u001b8Setting up aria2 (1.35.0-1build1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 88%]\u001b[49m\u001b[39m [###################################################.......] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 94%]\u001b[49m\u001b[39m [######################################################....] \u001b8Processing triggers for man-db (2.9.1-1) ...\n",
      "Processing triggers for libc-bin (2.31-0ubuntu9.9) ...\n",
      "\n",
      "\u001b7\u001b[0;24r\u001b8\u001b[1A\u001b[J\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-09 22:35:23.885203: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-09 22:35:25.081790: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Installation finished in 67 seconds.\n",
      "\n",
      "🔄 Downloading model...\n",
      "\u001b[35m[\u001b[0m#f0d436 1.7GiB/1.9GiB\u001b[36m(88%)\u001b[0m CN:16 DL:\u001b[32m212MiB\u001b[0m ETA:\u001b[33m1s\u001b[0m\u001b[35m]\u001b[0m\u001b[0m\n",
      "Download Results:\n",
      "gid   |stat|avg speed  |path/URI\n",
      "======+====+===========+=======================================================\n",
      "f0d436|\u001b[1;32mOK\u001b[0m  |   214MiB/s|//content/sd-v1-5-pruned-noema-fp16.safetensors\n",
      "\n",
      "Status Legend:\n",
      "(OK):download completed.\n",
      "\n",
      "\n",
      "📄 Config saved to /storage/loras/desipose/training_config.toml\n",
      "📄 Dataset config saved to /storage/loras/desipose/dataset_config.toml\n",
      "\n",
      "⭐ Starting trainer...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/safetensors/torch.py:98: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  with safe_open(filename, framework=\"pt\", device=device) as f:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "Loading settings from /storage/loras/desipose/training_config.toml...\n",
      "/storage/loras/desipose/training_config\n",
      "prepare tokenizer\n",
      "Downloading (…)olve/main/vocab.json: 100%|███| 961k/961k [00:00<00:00, 19.9MB/s]\n",
      "Downloading (…)olve/main/merges.txt: 100%|███| 525k/525k [00:00<00:00, 15.8MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████| 389/389 [00:00<00:00, 107kB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████| 905/905 [00:00<00:00, 265kB/s]\n",
      "update token length: 225\n",
      "Load dataset config from /storage/loras/desipose/dataset_config.toml\n",
      "prepare images.\n",
      "found directory /storage/loras/desipose/dataset contains 94 image files\n",
      "470 train images with repeating.\n",
      "0 reg images.\n",
      "no regularization images / 正則化画像が見つかりませんでした\n",
      "[Dataset 0]\n",
      "  batch_size: 2\n",
      "  resolution: (512, 512)\n",
      "  enable_bucket: True\n",
      "  min_bucket_reso: 256\n",
      "  max_bucket_reso: 1024\n",
      "  bucket_reso_steps: 64\n",
      "  bucket_no_upscale: False\n",
      "\n",
      "  [Subset 0 of Dataset 0]\n",
      "    image_dir: \"/storage/loras/desipose/dataset\"\n",
      "    image_count: 94\n",
      "    num_repeats: 5\n",
      "    shuffle_caption: True\n",
      "    keep_tokens: 1\n",
      "    caption_dropout_rate: 0.0\n",
      "    caption_dropout_every_n_epoches: 0\n",
      "    caption_tag_dropout_rate: 0.0\n",
      "    color_aug: False\n",
      "    flip_aug: False\n",
      "    face_crop_aug_range: None\n",
      "    random_crop: False\n",
      "    token_warmup_min: 1,\n",
      "    token_warmup_step: 0,\n",
      "    is_reg: False\n",
      "    class_tokens: desipose\n",
      "    caption_extension: \n",
      "\n",
      "\n",
      "[Dataset 0]\n",
      "loading image sizes.\n",
      "100%|█████████████████████████████████████████| 94/94 [00:00<00:00, 3796.96it/s]\n",
      "make buckets\n",
      "number of images (including repeats) / 各bucketの画像枚数（繰り返し回数を含む）\n",
      "bucket 0: resolution (320, 768), count: 5\n",
      "bucket 1: resolution (384, 640), count: 70\n",
      "bucket 2: resolution (448, 576), count: 150\n",
      "bucket 3: resolution (512, 512), count: 35\n",
      "bucket 4: resolution (576, 448), count: 100\n",
      "bucket 5: resolution (640, 384), count: 110\n",
      "mean ar error (without repeats): 0.07130119954305823\n",
      "prepare accelerator\n",
      "Using accelerator 0.15.0 or above.\n",
      "loading model for process 0/1\n",
      "load StableDiffusion checkpoint\n",
      "/usr/local/lib/python3.9/dist-packages/safetensors/torch.py:98: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  with safe_open(filename, framework=\"pt\", device=device) as f:\n",
      "loading u-net: <All keys matched successfully>\n",
      "loading vae: <All keys matched successfully>\n",
      "Downloading (…)lve/main/config.json: 100%|██| 4.52k/4.52k [00:00<00:00, 517kB/s]\n",
      "Downloading (…)\"pytorch_model.bin\";: 100%|██| 1.71G/1.71G [00:06<00:00, 284MB/s]\n",
      "loading text encoder: <All keys matched successfully>\n",
      "Replace CrossAttention.forward to use xformers\n",
      "[Dataset 0]\n",
      "caching latents.\n",
      "100%|███████████████████████████████████████████| 94/94 [00:18<00:00,  5.13it/s]\n",
      "import network module: networks.lora\n",
      "create LoRA network. base dim (rank): 16, alpha: 8\n",
      "create LoRA for Text Encoder: 72 modules.\n",
      "create LoRA for U-Net: 192 modules.\n",
      "enable LoRA for text encoder\n",
      "enable LoRA for U-Net\n",
      "prepare optimizer, data loader etc.\n",
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "For effortless bug reporting copy-paste your error into this form: https://docs.google.com/forms/d/e/1FAIpQLScPB8emS3Thkp66nvqwmjTEgxp8Y9ufuWTzFyr9kJ5AoI47dQ/viewform?usp=sf_link\n",
      "================================================================================\n",
      "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/paths.py:27: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/lib/python3.9/dist-packages/cv2/../../lib64')}\n",
      "  warn(\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda-11.6/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 6.1\n",
      "CUDA SETUP: Detected CUDA version 116\n",
      "CUDA SETUP: Loading binary /usr/local/lib/python3.9/dist-packages/bitsandbytes/libbitsandbytes_cuda116_nocublaslt.so...\n",
      "use 8-bit AdamW optimizer | {}\n",
      "override steps. steps for 20 epochs is / 指定エポックまでのステップ数: 4720\n",
      "running training / 学習開始\n",
      "  num train images * repeats / 学習画像の数×繰り返し回数: 470\n",
      "  num reg images / 正則化画像の数: 0\n",
      "  num batches per epoch / 1epochのバッチ数: 236\n",
      "  num epochs / epoch数: 20\n",
      "  batch size per device / バッチサイズ: 2\n",
      "  gradient accumulation steps / 勾配を合計するステップ数 = 1\n",
      "  total optimization steps / 学習ステップ数: 4720\n",
      "steps:   0%|                                           | 0/4720 [00:00<?, ?it/s]epoch 1/20\n",
      "steps:   5%|▉                  | 236/4720 [06:22<2:01:10,  1.62s/it, loss=0.105]saving checkpoint: /storage/loras/desipose/output/desipose-01.safetensors\n",
      "epoch 2/20\n",
      "steps:  10%|█▉                 | 472/4720 [12:51<1:55:42,  1.63s/it, loss=0.112]saving checkpoint: /storage/loras/desipose/output/desipose-02.safetensors\n",
      "epoch 3/20\n",
      "steps:  15%|██▊                | 708/4720 [19:19<1:49:31,  1.64s/it, loss=0.103]saving checkpoint: /storage/loras/desipose/output/desipose-03.safetensors\n",
      "epoch 4/20\n",
      "steps:  20%|███▊               | 944/4720 [25:47<1:43:11,  1.64s/it, loss=0.101]saving checkpoint: /storage/loras/desipose/output/desipose-04.safetensors\n",
      "epoch 5/20\n",
      "steps:  25%|████▌             | 1180/4720 [32:16<1:36:48,  1.64s/it, loss=0.105]saving checkpoint: /storage/loras/desipose/output/desipose-05.safetensors\n",
      "epoch 6/20\n",
      "steps:  30%|█████▍            | 1416/4720 [38:44<1:30:23,  1.64s/it, loss=0.102]saving checkpoint: /storage/loras/desipose/output/desipose-06.safetensors\n",
      "epoch 7/20\n",
      "steps:  35%|█████▉           | 1652/4720 [45:12<1:23:56,  1.64s/it, loss=0.0981]saving checkpoint: /storage/loras/desipose/output/desipose-07.safetensors\n",
      "epoch 8/20\n",
      "steps:  40%|███████▏          | 1888/4720 [51:39<1:17:29,  1.64s/it, loss=0.104]saving checkpoint: /storage/loras/desipose/output/desipose-08.safetensors\n",
      "epoch 9/20\n",
      "steps:  45%|████████          | 2124/4720 [58:07<1:11:02,  1.64s/it, loss=0.102]saving checkpoint: /storage/loras/desipose/output/desipose-09.safetensors\n",
      "epoch 10/20\n",
      "steps:  50%|████████        | 2360/4720 [1:04:35<1:04:35,  1.64s/it, loss=0.104]saving checkpoint: /storage/loras/desipose/output/desipose-10.safetensors\n",
      "epoch 11/20\n",
      "steps:  55%|█████████▉        | 2596/4720 [1:11:02<58:07,  1.64s/it, loss=0.101]saving checkpoint: /storage/loras/desipose/output/desipose-11.safetensors\n",
      "removing old checkpoint: /storage/loras/desipose/output/desipose-01.safetensors\n",
      "epoch 12/20\n",
      "steps:  57%|█████████▊       | 2711/4720 [1:14:12<54:59,  1.64s/it, loss=0.0979]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import toml\n",
    "import shutil\n",
    "import zipfile\n",
    "from time import time\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "\n",
    "# These carry information from past executions\n",
    "if \"model_url\" in globals():\n",
    "  old_model_url = model_url\n",
    "else:\n",
    "  old_model_url = None\n",
    "if \"dependencies_installed\" not in globals():\n",
    "  dependencies_installed = False\n",
    "if \"model_file\" not in globals():\n",
    "  model_file = None\n",
    "\n",
    "# These may be set by other cells, some are legacy\n",
    "if \"custom_dataset\" not in globals():\n",
    "  custom_dataset = None\n",
    "if \"override_dataset_config_file\" not in globals():\n",
    "  override_dataset_config_file = None\n",
    "if \"override_config_file\" not in globals():\n",
    "  override_config_file = None\n",
    "if \"optimizer\" not in globals():\n",
    "  optimizer = \"AdamW8bit\"\n",
    "if \"optimizer_args\" not in globals():\n",
    "  optimizer_args = None\n",
    "if \"continue_from_lora\" not in globals():\n",
    "  continue_from_lora = \"\"\n",
    "if \"weighted_captions\" not in globals():\n",
    "  weighted_captions = False\n",
    "if \"adjust_tags\" not in globals():\n",
    "  adjust_tags = False\n",
    "if \"keep_tokens_weight\" not in globals():\n",
    "  keep_tokens_weight = 1.0\n",
    "\n",
    "\n",
    "print(\"project_name \"+project_name)\n",
    "print(\"folder_structure \"+folder_structure)\n",
    "print(\"main_dir \"+main_dir)\n",
    "print(\"config_folder \"+config_folder)\n",
    "print(\"images_folder \"+images_folder)\n",
    "#print(\"output_folder \"+output_folder)\n",
    "\n",
    "\n",
    "\n",
    "COLAB = False # low ram\n",
    "COMMIT = \"e6ad3cbc66130fdc3bf9ecd1e0272969b1d613f7\"\n",
    "BETTER_EPOCH_NAMES = True\n",
    "LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "#@title ## 🚩 Start Here\n",
    "\n",
    "#@markdown ### ▶️ Setup\n",
    "#@markdown Your project name will be the same as the folder containing your images. Spaces aren't allowed.\n",
    "#project_name = \"maxicanlust-maritza-mendez\" #@param {type:\"string\"}\n",
    "#@markdown The folder structure doesn't matter and is purely for comfort. Make sure to always pick the same one. I like organizing by project.\n",
    "#folder_structure = \"Organize by project (MyDrive/Loras/project_name/dataset)\" #@param [\"Organize by category (MyDrive/lora_training/datasets/project_name)\", \"Organize by project (MyDrive/Loras/project_name/dataset)\"]\n",
    "#@markdown Decide the model that will be downloaded and used for training. These options should produce clean and consistent results. You can also choose your own by pasting its download link.\n",
    "training_model = \"Stable Diffusion (sd-v1-5-pruned-noema-fp16.safetensors)\" #@param [\"Anime (animefull-final-pruned-fp16.safetensors)\", \"AnyLora (AnyLoRA_noVae_fp16-pruned.ckpt)\", \"Stable Diffusion (sd-v1-5-pruned-noema-fp16.safetensors)\"]\n",
    "optional_custom_training_model_url = \"\" #@param {type:\"string\"}\n",
    "custom_model_is_based_on_sd2 = False #@param {type:\"boolean\"}\n",
    "\n",
    "if optional_custom_training_model_url:\n",
    "  model_url = optional_custom_training_model_url\n",
    "elif \"AnyLora\" in training_model:\n",
    "  model_url = \"https://huggingface.co/Lykon/AnyLoRA/resolve/main/AnyLoRA_noVae_fp16-pruned.ckpt\"\n",
    "elif \"Anime\" in training_model:\n",
    "  model_url = \"https://huggingface.co/hollowstrawberry/stable-diffusion-guide/resolve/main/models/animefull-final-pruned-fp16.safetensors\"\n",
    "else:\n",
    "  model_url = \"https://huggingface.co/hollowstrawberry/stable-diffusion-guide/resolve/main/models/sd-v1-5-pruned-noema-fp16.safetensors\"\n",
    "\n",
    "#@markdown ### ▶️ Processing\n",
    "#@markdown Resolution of 512 is standard for Stable Diffusion 1.5. Higher resolution training is much slower but can lead to better details. <p>\n",
    "#@markdown Images will be automatically scaled while training to produce the best results, so you don't need to crop or resize anything yourself.\n",
    "resolution = 512 #@param {type:\"slider\", min:512, max:1024, step:128}\n",
    "#@markdown This option will train your images both normally and flipped, for no extra cost, to learn more from them. Turn it on specially if you have less than 20 images. <p>\n",
    "#@markdown **Turn it off if you care about asymmetrical elements in your Lora**.\n",
    "flip_aug = False #@param {type:\"boolean\"}\n",
    "#markdown Leave empty for no captions.\n",
    "caption_extension = \".txt\" #param {type:\"string\"}\n",
    "#@markdown Shuffling anime tags in place improves learning and prompting. An activation tag goes at the start of every text file and will not be shuffled.\n",
    "shuffle_tags = True #@param {type:\"boolean\"}\n",
    "shuffle_caption = shuffle_tags\n",
    "activation_tags = \"1\" #@param [0,1,2,3]\n",
    "keep_tokens = int(activation_tags)\n",
    "\n",
    "#@markdown ### ▶️ Steps <p>\n",
    "#@markdown Your images will repeat this number of times during training. I recommend that your images multiplied by their repeats is between 200 and 400.\n",
    "num_repeats = my_per_image_repeat #@param {type:\"number\"}\n",
    "#@markdown Choose how long you want to train for. A good starting point is around 10 epochs or around 2000 steps.<p>\n",
    "#@markdown One epoch is a number of steps equal to: your number of images multiplied by their repeats, divided by batch size. <p>\n",
    "preferred_unit = \"Epochs\" #@param [\"Epochs\", \"Steps\"]\n",
    "how_many = my_net_epoch #@param {type:\"number\"}\n",
    "max_train_epochs = how_many if preferred_unit == \"Epochs\" else None\n",
    "max_train_steps = how_many if preferred_unit == \"Steps\" else None\n",
    "#@markdown Saving more epochs will let you compare your Lora's progress better.\n",
    "save_every_n_epochs = 1 #@param {type:\"number\"}\n",
    "keep_only_last_n_epochs = 10 #@param {type:\"number\"}\n",
    "if not save_every_n_epochs:\n",
    "  save_every_n_epochs = max_train_epochs\n",
    "if not keep_only_last_n_epochs:\n",
    "  keep_only_last_n_epochs = max_train_epochs\n",
    "#@markdown Increasing the batch size makes training faster, but may make learning worse. Recommended 2 or 3.\n",
    "train_batch_size = 2 #@param {type:\"slider\", min:1, max:8, step:1}\n",
    "\n",
    "#@markdown ### ▶️ Learning\n",
    "#@markdown The learning rate is the most important for your results. If you want to train slower with lots of images, or if your dim and alpha are high, move the unet to 2e-4 or lower. <p>\n",
    "#@markdown The text encoder helps your Lora learn concepts slightly better. It is recommended to make it half or a fifth of the unet. If you're training a style you can even set it to 0.\n",
    "unet_lr = 5e-4 #@param {type:\"number\"}\n",
    "text_encoder_lr = 1e-4 #@param {type:\"number\"}\n",
    "#@markdown The scheduler is the algorithm that guides the learning rate. If you're not sure, pick `constant` and ignore the number. I personally recommend `cosine_with_restarts` with 3 restarts.\n",
    "lr_scheduler = \"cosine_with_restarts\" #@param [\"constant\", \"cosine\", \"cosine_with_restarts\", \"constant_with_warmup\", \"linear\", \"polynomial\"]\n",
    "lr_scheduler_number = 3 #@param {type:\"number\"}\n",
    "lr_scheduler_num_cycles = lr_scheduler_number if lr_scheduler == \"cosine_with_restarts\" else 0\n",
    "lr_scheduler_power = lr_scheduler_number if lr_scheduler == \"polynomial\" else 0\n",
    "#@markdown Steps spent \"warming up\" the learning rate during training for efficiency. I recommend leaving it at 5%.\n",
    "lr_warmup_ratio = 0.05 #@param {type:\"slider\", min:0.0, max:0.5, step:0.01}\n",
    "lr_warmup_steps = 0\n",
    "#@markdown New feature that adjusts loss over time, makes learning much more efficient, and training can be done with about half as many epochs. Uses a value of 5.0 as recommended by [the paper](https://arxiv.org/abs/2303.09556).\n",
    "min_snr_gamma = True #@param {type:\"boolean\"}\n",
    "min_snr_gamma_value = 5.0 if min_snr_gamma else None\n",
    "\n",
    "#@markdown ### ▶️ Structure\n",
    "#@markdown LoRA is the classic type, while LoCon is good with styles. Lycoris require [this extension](https://github.com/KohakuBlueleaf/a1111-sd-webui-lycoris) for webui to work like normal loras. More info [here](https://github.com/KohakuBlueleaf/Lycoris).\n",
    "lora_type = \"LoRA\" #@param [\"LoRA\", \"LoCon Lycoris\", \"LoHa Lycoris\"]\n",
    "\n",
    "#@markdown Below are some recommended values for the following settings:\n",
    "\n",
    "#@markdown | type | network_dim | network_alpha | conv_dim | conv_alpha |\n",
    "#@markdown | :---: | :---: | :---: | :---: | :---: |\n",
    "#@markdown | LoRA | 32 | 16 |   |   |\n",
    "#@markdown | LoCon | 16 | 8 | 8 | 1 |\n",
    "#@markdown | LoHa | 8 | 4 | 4 | 1 |\n",
    "\n",
    "#@markdown More dim means larger Lora, it can hold more information but more isn't always better. A dim between 8-32 is recommended, and alpha equal to half the dim.\n",
    "network_dim = 16 #@param {type:\"slider\", min:1, max:128, step:1}\n",
    "network_alpha = 8 #@param {type:\"slider\", min:1, max:128, step:1}\n",
    "#@markdown The following values don't affect LoRA. They work like dim/alpha but only for the additional learning layers of Lycoris.\n",
    "conv_dim = 8 #@param {type:\"slider\", min:1, max:64, step:1}\n",
    "conv_alpha = 1 #@param {type:\"slider\", min:1, max:64, step:1}\n",
    "conv_compression = False #@param {type:\"boolean\"}\n",
    "\n",
    "network_module = \"lycoris.kohya\" if \"Lycoris\" in lora_type else \"networks.lora\"\n",
    "network_args = None if lora_type == \"LoRA\" else [\n",
    "  f\"conv_dim={conv_dim}\",\n",
    "  f\"conv_alpha={conv_alpha}\",\n",
    "]\n",
    "if \"Lycoris\" in lora_type:\n",
    "  network_args.append(f\"algo={'loha' if 'LoHa' in lora_type else 'lora'}\")\n",
    "  network_args.append(f\"disable_conv_cp={str(not conv_compression)}\")\n",
    "\n",
    "#markdown ### ▶️ Experimental\n",
    "#markdown Save additional data equaling ~1 GB allowing you to resume training later.\n",
    "save_state = False #param {type:\"boolean\"}\n",
    "#markdown Resume training if a save state is found.\n",
    "resume = False #param {type:\"boolean\"}\n",
    "\n",
    "#@markdown ### ▶️ Ready\n",
    "#@markdown You can now run this cell to cook your Lora. Good luck! <p>\n",
    "\n",
    "\n",
    "# 👩‍💻 Cool code goes here\n",
    "\n",
    "if optimizer == \"DAdaptation\":\n",
    "  optimizer_args = [\"decouple=True\",\"weight_decay=0.02\",\"betas=[0.9,0.99]\"]\n",
    "  unet_lr = 0.5\n",
    "  text_encoder_lr = 0.5\n",
    "  lr_scheduler = \"constant_with_warmup\"\n",
    "  network_alpha = network_dim\n",
    "\n",
    "#root_dir = \"/content\" if COLAB else \"~/Loras\"\n",
    "deps_dir = os.path.join(root_dir, \"deps\")\n",
    "repo_dir = os.path.join(root_dir, \"kohya-trainer\")\n",
    "\n",
    "#main_dir      = os.path.join(root_dir, \"drive/MyDrive/Loras\") if COLAB else root_dir\n",
    "log_folder    = os.path.join(main_dir, \"_logs\")\n",
    "#config_folder = os.path.join(main_dir, project_name)\n",
    "#images_folder = os.path.join(main_dir, project_name, \"dataset\")\n",
    "output_folder = os.path.join(main_dir, project_name, \"output\")\n",
    "\n",
    "config_file = os.path.join(config_folder, \"training_config.toml\")\n",
    "dataset_config_file = os.path.join(config_folder, \"dataset_config.toml\")\n",
    "accelerate_config_file = os.path.join(repo_dir, \"accelerate_config/config.yaml\")\n",
    "\n",
    "print(\"project_name \"+project_name)\n",
    "print(\"folder_structure \"+folder_structure)\n",
    "print(\"main_dir \"+main_dir)\n",
    "print(\"config_folder \"+config_folder)\n",
    "print(\"images_folder \"+images_folder)\n",
    "print(\"output_folder \"+output_folder)\n",
    "\n",
    "def clone_repo():\n",
    "  os.chdir(root_dir)\n",
    "  !git clone https://github.com/kohya-ss/sd-scripts {repo_dir}\n",
    "  os.chdir(repo_dir)\n",
    "  if COMMIT:\n",
    "    !git reset --hard {COMMIT}\n",
    "  !wget https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/requirements.txt -q -O requirements.txt\n",
    "\n",
    "def install_dependencies():\n",
    "  clone_repo()\n",
    "  !apt -y update -qq\n",
    "  !apt -y install aria2 -qq\n",
    "  !pip -q install --upgrade -r requirements.txt --extra-index-url https://download.pytorch.org/whl/cu118\n",
    "\n",
    "  # patch kohya for minor stuff\n",
    "  if COLAB:\n",
    "    !sed -i \"s@cpu@cuda@\" library/model_util.py # low ram\n",
    "  if LOAD_TRUNCATED_IMAGES:\n",
    "    !sed -i 's/from PIL import Image/from PIL import Image, ImageFile\\nImageFile.LOAD_TRUNCATED_IMAGES=True/g' library/train_util.py # fix truncated jpegs error\n",
    "  if BETTER_EPOCH_NAMES:\n",
    "    !sed -i 's/{:06d}/{:02d}/g' library/train_util.py # make epoch names shorter\n",
    "    !sed -i 's/\".\" + args.save_model_as)/\"-{:02d}.\".format(num_train_epochs) + args.save_model_as)/g' train_network.py # name of the last epoch will match the rest\n",
    "\n",
    "  from accelerate.utils import write_basic_config\n",
    "  if not os.path.exists(accelerate_config_file):\n",
    "    write_basic_config(save_location=accelerate_config_file)\n",
    "\n",
    "  os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "  os.environ[\"BITSANDBYTES_NOWELCOME\"] = \"1\"\n",
    "  os.environ[\"SAFETENSORS_FAST_GPU\"] = \"1\"\n",
    "\n",
    "def validate_dataset():\n",
    "  global lr_warmup_steps, lr_warmup_ratio, caption_extension, keep_tokens, keep_tokens_weight, weighted_captions, adjust_tags\n",
    "  supported_types = (\".png\", \".jpg\", \".jpeg\", \".webp\", \".bmp\")\n",
    "\n",
    "  print(\"\\n💿 Checking dataset...\")\n",
    "  if not project_name.strip() or any(c in project_name for c in \" .()\\\"'\\\\/\"):\n",
    "    print(\"💥 Error: Please choose a valid project name.\")\n",
    "    return\n",
    "\n",
    "  if custom_dataset:\n",
    "    try:\n",
    "      datconf = toml.loads(custom_dataset)\n",
    "      datasets = [d for d in datconf[\"datasets\"][0][\"subsets\"]]\n",
    "    except:\n",
    "      print(f\"💥 Error: Your custom dataset is invalid or contains an error! Please check the original template.\")\n",
    "      return\n",
    "    reg = [d for d in datasets if d.get(\"is_reg\", False)]\n",
    "    for r in reg:\n",
    "      print(\"📁\"+r[\"image_dir\"].replace(\"/content/drive/\", \"\") + \" (Regularization)\")\n",
    "    datasets = [d for d in datasets if d not in reg]\n",
    "    datasets_dict = {d[\"image_dir\"]: d[\"num_repeats\"] for d in datasets}\n",
    "    folders = datasets_dict.keys()\n",
    "    files = [f for folder in folders for f in os.listdir(folder)]\n",
    "    images_repeats = {folder: (len([f for f in os.listdir(folder) if f.lower().endswith(supported_types)]), datasets_dict[folder]) for folder in folders}\n",
    "  else:\n",
    "    folders = [images_folder]\n",
    "    files = os.listdir(images_folder)\n",
    "    images_repeats = {images_folder: (len([f for f in files if f.lower().endswith(supported_types)]), num_repeats)}\n",
    "\n",
    "  for folder in folders:\n",
    "    if not os.path.exists(folder):\n",
    "      print(f\"💥 Error: The folder {folder.replace('/content/drive/', '')} doesn't exist.\")\n",
    "      return\n",
    "  for folder, (img, rep) in images_repeats.items():\n",
    "    if not img:\n",
    "      print(f\"💥 Error: Your {folder.replace('/content/drive/', '')} folder is empty.\")\n",
    "      return\n",
    "  for f in files:\n",
    "    if not f.lower().endswith(\".txt\") and not f.lower().endswith(supported_types):\n",
    "      print(f\"💥 Error: Invalid file in dataset: \\\"{f}\\\". Aborting.\")\n",
    "      return\n",
    "\n",
    "  if not [txt for txt in files if txt.lower().endswith(\".txt\")]:\n",
    "    caption_extension = \"\"\n",
    "  if continue_from_lora and not (continue_from_lora.endswith(\".safetensors\") and os.path.exists(continue_from_lora)):\n",
    "    print(f\"💥 Error: Invalid path to existing Lora. Example: /content/drive/MyDrive/Loras/example.safetensors\")\n",
    "    return\n",
    "\n",
    "  pre_steps_per_epoch = sum(img*rep for (img, rep) in images_repeats.values())\n",
    "  steps_per_epoch = pre_steps_per_epoch/train_batch_size\n",
    "  total_steps = max_train_steps or int(max_train_epochs*steps_per_epoch)\n",
    "  estimated_epochs = int(total_steps/steps_per_epoch)\n",
    "  lr_warmup_steps = int(total_steps*lr_warmup_ratio)\n",
    "\n",
    "  for folder, (img, rep) in images_repeats.items():\n",
    "    print(\"📁\"+folder.replace(\"/content/drive/\", \"\"))\n",
    "    print(f\"📈 Found {img} images with {rep} repeats, equaling {img*rep} steps.\")\n",
    "  print(f\"📉 Divide {pre_steps_per_epoch} steps by {train_batch_size} batch size to get {steps_per_epoch} steps per epoch.\")\n",
    "  if max_train_epochs:\n",
    "    print(f\"🔮 There will be {max_train_epochs} epochs, for around {total_steps} total training steps.\")\n",
    "  else:\n",
    "    print(f\"🔮 There will be {total_steps} steps, divided into {estimated_epochs} epochs and then some.\")\n",
    "\n",
    "  if total_steps > 10000:\n",
    "    print(\"💥 Error: Your total steps are too high. You probably made a mistake. Aborting...\")\n",
    "    return\n",
    "\n",
    "  if adjust_tags:\n",
    "    print(f\"\\n📎 Weighted tags: {'ON' if weighted_captions else 'OFF'}\")\n",
    "    if weighted_captions:\n",
    "      print(f\"📎 Will use {keep_tokens_weight} weight on {keep_tokens} activation tag(s)\")\n",
    "    print(\"📎 Adjusting tags...\")\n",
    "    adjust_weighted_tags(folders, keep_tokens, keep_tokens_weight, weighted_captions)\n",
    "\n",
    "  return True\n",
    "\n",
    "def adjust_weighted_tags(folders, keep_tokens: int, keep_tokens_weight: float, weighted_captions: bool):\n",
    "  weighted_tag = re.compile(r\"\\((.+?):[.\\d]+\\)(,|$)\")\n",
    "  for folder in folders:\n",
    "    for txt in [f for f in os.listdir(folder) if f.lower().endswith(\".txt\")]:\n",
    "      with open(os.path.join(folder, txt), 'r') as f:\n",
    "        content = f.read()\n",
    "      # reset previous changes\n",
    "      content = content.replace('\\\\', '')\n",
    "      content = weighted_tag.sub(r'\\1\\2', content)\n",
    "      if weighted_captions:\n",
    "        # re-apply changes\n",
    "        content = content.replace(r'(', r'\\(').replace(r')', r'\\)').replace(r':', r'\\:')\n",
    "        if keep_tokens_weight > 1:\n",
    "          tags = [s.strip() for s in content.split(\",\")]\n",
    "          for i in range(min(keep_tokens, len(tags))):\n",
    "            tags[i] = f'({tags[i]}:{keep_tokens_weight})'\n",
    "          content = \", \".join(tags)\n",
    "      with open(os.path.join(folder, txt), 'w') as f:\n",
    "        f.write(content)\n",
    "\n",
    "def create_config():\n",
    "  global dataset_config_file, config_file, model_file\n",
    "\n",
    "  if resume:\n",
    "    resume_points = [f.path for f in os.scandir(output_folder) if f.is_dir()]\n",
    "    resume_points.sort()\n",
    "    last_resume_point = resume_points[-1] if resume_points else None\n",
    "  else:\n",
    "    last_resume_point = None\n",
    "\n",
    "  if override_config_file:\n",
    "    config_file = override_config_file\n",
    "    print(f\"\\n⭕ Using custom config file {config_file}\")\n",
    "  else:\n",
    "    config_dict = {\n",
    "      \"additional_network_arguments\": {\n",
    "        \"unet_lr\": unet_lr,\n",
    "        \"text_encoder_lr\": text_encoder_lr,\n",
    "        \"network_dim\": network_dim,\n",
    "        \"network_alpha\": network_alpha,\n",
    "        \"network_module\": network_module,\n",
    "        \"network_args\": network_args,\n",
    "        \"network_train_unet_only\": True if text_encoder_lr == 0 else None,\n",
    "        \"network_weights\": continue_from_lora if continue_from_lora else None\n",
    "      },\n",
    "      \"optimizer_arguments\": {\n",
    "        \"learning_rate\": unet_lr,\n",
    "        \"lr_scheduler\": lr_scheduler,\n",
    "        \"lr_scheduler_num_cycles\": lr_scheduler_num_cycles if lr_scheduler == \"cosine_with_restarts\" else None,\n",
    "        \"lr_scheduler_power\": lr_scheduler_power if lr_scheduler == \"polynomial\" else None,\n",
    "        \"lr_warmup_steps\": lr_warmup_steps if lr_scheduler != \"constant\" else None,\n",
    "        \"optimizer_type\": optimizer,\n",
    "        \"optimizer_args\": optimizer_args if optimizer_args else None,\n",
    "      },\n",
    "      \"training_arguments\": {\n",
    "        \"max_train_steps\": max_train_steps,\n",
    "        \"max_train_epochs\": max_train_epochs,\n",
    "        \"save_every_n_epochs\": save_every_n_epochs,\n",
    "        \"save_last_n_epochs\": keep_only_last_n_epochs,\n",
    "        \"train_batch_size\": train_batch_size,\n",
    "        \"noise_offset\": None,\n",
    "        \"clip_skip\": 2,\n",
    "        \"min_snr_gamma\": min_snr_gamma_value,\n",
    "        \"weighted_captions\": weighted_captions,\n",
    "        \"seed\": 42,\n",
    "        \"max_token_length\": 225,\n",
    "        \"xformers\": True,\n",
    "        \"lowram\": COLAB,\n",
    "        \"max_data_loader_n_workers\": 8,\n",
    "        \"persistent_data_loader_workers\": True,\n",
    "        \"save_precision\": \"fp16\",\n",
    "        \"mixed_precision\": \"fp16\",\n",
    "        \"output_dir\": output_folder,\n",
    "        \"logging_dir\": log_folder,\n",
    "        \"output_name\": project_name,\n",
    "        \"log_prefix\": project_name,\n",
    "        \"save_state\": save_state,\n",
    "        \"save_last_n_epochs_state\": 1 if save_state else None,\n",
    "        \"resume\": last_resume_point\n",
    "      },\n",
    "      \"model_arguments\": {\n",
    "        \"pretrained_model_name_or_path\": model_file,\n",
    "        \"v2\": custom_model_is_based_on_sd2,\n",
    "        \"v_parameterization\": True if custom_model_is_based_on_sd2 else None,\n",
    "      },\n",
    "      \"saving_arguments\": {\n",
    "        \"save_model_as\": \"safetensors\",\n",
    "      },\n",
    "      \"dreambooth_arguments\": {\n",
    "        \"prior_loss_weight\": 1.0,\n",
    "      },\n",
    "      \"dataset_arguments\": {\n",
    "        \"cache_latents\": True,\n",
    "      },\n",
    "    }\n",
    "\n",
    "    for key in config_dict:\n",
    "      if isinstance(config_dict[key], dict):\n",
    "        config_dict[key] = {k: v for k, v in config_dict[key].items() if v is not None}\n",
    "\n",
    "    with open(config_file, \"w\") as f:\n",
    "      f.write(toml.dumps(config_dict))\n",
    "    print(f\"\\n📄 Config saved to {config_file}\")\n",
    "\n",
    "  if override_dataset_config_file:\n",
    "    dataset_config_file = override_dataset_config_file\n",
    "    print(f\"⭕ Using custom dataset config file {dataset_config_file}\")\n",
    "  else:\n",
    "    dataset_config_dict = {\n",
    "      \"general\": {\n",
    "        \"resolution\": resolution,\n",
    "        \"shuffle_caption\": shuffle_caption,\n",
    "        \"keep_tokens\": keep_tokens,\n",
    "        \"flip_aug\": flip_aug,\n",
    "        \"caption_extension\": caption_extension,\n",
    "        \"enable_bucket\": True,\n",
    "        \"bucket_reso_steps\": 64,\n",
    "        \"bucket_no_upscale\": False,\n",
    "        \"min_bucket_reso\": 320 if resolution > 640 else 256,\n",
    "        \"max_bucket_reso\": 1280 if resolution > 640 else 1024,\n",
    "      },\n",
    "      \"datasets\": toml.loads(custom_dataset)[\"datasets\"] if custom_dataset else [\n",
    "        {\n",
    "          \"subsets\": [\n",
    "            {\n",
    "              \"num_repeats\": num_repeats,\n",
    "              \"image_dir\": images_folder,\n",
    "              \"class_tokens\": None if caption_extension else project_name\n",
    "            }\n",
    "          ]\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "\n",
    "    for key in dataset_config_dict:\n",
    "      if isinstance(dataset_config_dict[key], dict):\n",
    "        dataset_config_dict[key] = {k: v for k, v in dataset_config_dict[key].items() if v is not None}\n",
    "\n",
    "    with open(dataset_config_file, \"w\") as f:\n",
    "      f.write(toml.dumps(dataset_config_dict))\n",
    "    print(f\"📄 Dataset config saved to {dataset_config_file}\")\n",
    "\n",
    "def download_model():\n",
    "  global old_model_url, model_url, model_file\n",
    "  real_model_url = model_url.strip()\n",
    "\n",
    "  if real_model_url.lower().endswith((\".ckpt\", \".safetensors\")):\n",
    "    model_file = f\"/content{real_model_url[real_model_url.rfind('/'):]}\"\n",
    "  else:\n",
    "    model_file = \"/content/downloaded_model.safetensors\"\n",
    "    if os.path.exists(model_file):\n",
    "      !rm \"{model_file}\"\n",
    "\n",
    "  if m := re.search(r\"(?:https?://)?(?:www\\.)?huggingface\\.co/[^/]+/[^/]+/blob\", model_url):\n",
    "    real_model_url = real_model_url.replace(\"blob\", \"resolve\")\n",
    "  elif m := re.search(r\"(?:https?://)?(?:www\\.)?civitai\\.com/models/([0-9]+)\", model_url):\n",
    "    real_model_url = f\"https://civitai.com/api/download/models/{m.group(1)}\"\n",
    "\n",
    "  !aria2c \"{real_model_url}\" --console-log-level=warn -c -s 16 -x 16 -k 10M -d / -o \"{model_file}\"\n",
    "\n",
    "  if model_file.lower().endswith(\".safetensors\"):\n",
    "    from safetensors.torch import load_file as load_safetensors\n",
    "    try:\n",
    "      test = load_safetensors(model_file)\n",
    "      del test\n",
    "    except Exception as e:\n",
    "      #if \"HeaderTooLarge\" in str(e):\n",
    "      new_model_file = os.path.splitext(model_file)[0]+\".ckpt\"\n",
    "      !mv \"{model_file}\" \"{new_model_file}\"\n",
    "      model_file = new_model_file\n",
    "      print(f\"Renamed model to {os.path.splitext(model_file)[0]}.ckpt\")\n",
    "\n",
    "  if model_file.lower().endswith(\".ckpt\"):\n",
    "    from torch import load as load_ckpt\n",
    "    try:\n",
    "      test = load_ckpt(model_file)\n",
    "      del test\n",
    "    except Exception as e:\n",
    "      return False\n",
    "\n",
    "  return True\n",
    "\n",
    "def main():\n",
    "  global dependencies_installed\n",
    "\n",
    "  for dir in (main_dir, deps_dir, repo_dir, log_folder, images_folder, output_folder, config_folder):\n",
    "    os.makedirs(dir, exist_ok=True)\n",
    "\n",
    "  if not validate_dataset():\n",
    "    return\n",
    "\n",
    "  if not dependencies_installed:\n",
    "    print(\"\\n🏭 Installing dependencies...\\n\")\n",
    "    t0 = time()\n",
    "    install_dependencies()\n",
    "    t1 = time()\n",
    "    dependencies_installed = True\n",
    "    print(f\"\\n✅ Installation finished in {int(t1-t0)} seconds.\")\n",
    "  else:\n",
    "    print(\"\\n✅ Dependencies already installed.\")\n",
    "\n",
    "  if old_model_url != model_url or not model_file or not os.path.exists(model_file):\n",
    "    print(\"\\n🔄 Downloading model...\")\n",
    "    if not download_model():\n",
    "      print(\"\\n💥 Error: The model you selected is invalid or corrupted, or couldn't be downloaded. You can use a civitai or huggingface link, or any direct download link.\")\n",
    "      return\n",
    "    print()\n",
    "  else:\n",
    "    print(\"\\n🔄 Model already downloaded.\\n\")\n",
    "\n",
    "  create_config()\n",
    "\n",
    "  print(\"\\n⭐ Starting trainer...\\n\")\n",
    "  os.chdir(repo_dir)\n",
    "\n",
    "  !accelerate launch --config_file={accelerate_config_file} --num_cpu_threads_per_process=1 train_network.py --dataset_config={dataset_config_file} --config_file={config_file}\n",
    "\n",
    "  if not get_ipython().__dict__['user_ns']['_exit_code']:\n",
    "    display(Markdown(\"### ✅ Done! [Go download your Lora(s) from Google Drive](https://drive.google.com/drive/my-drive)\"))\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gDB9GXRONfiU"
   },
   "source": [
    "## *️⃣ Extras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "cellView": "form",
    "execution": {
     "iopub.execute_input": "2023-08-10T18:38:30.231564Z",
     "iopub.status.busy": "2023-08-10T18:38:30.230643Z",
     "iopub.status.idle": "2023-08-10T18:38:30.239399Z",
     "shell.execute_reply": "2023-08-10T18:38:30.238420Z",
     "shell.execute_reply.started": "2023-08-10T18:38:30.231533Z"
    },
    "id": "xEsqOglcc6hA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Top 50 tags:\n"
     ]
    }
   ],
   "source": [
    "if \"step1_installed_flag\" not in globals():\n",
    "  raise Exception(\"Please run step 1 first!\")\n",
    "  \n",
    "#@markdown ### 📈 Analyze Tags\n",
    "#@markdown Perhaps you need another look at your dataset.\n",
    "show_top_tags = 50 #@param {type:\"number\"}\n",
    "\n",
    "from collections import Counter\n",
    "top_tags = Counter()\n",
    "\n",
    "for txt in [f for f in os.listdir(images_folder) if f.lower().endswith(\".txt\")]:\n",
    "  with open(os.path.join(images_folder, txt), 'r') as f:\n",
    "    top_tags.update([s.strip() for s in f.read().split(\",\")])\n",
    "\n",
    "top_tags = Counter(top_tags)\n",
    "print(f\"📊 Top {show_top_tags} tags:\")\n",
    "for k, v in top_tags.most_common(show_top_tags):\n",
    "  print(f\"{k} ({v})\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
